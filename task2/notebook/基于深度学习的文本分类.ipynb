{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "680FN7pijGx3"
   },
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:45:16.354423Z",
     "start_time": "2020-08-10T00:45:16.340430Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "30n6pfOEjGx5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dn7U_kimjGyF"
   },
   "source": [
    "## 2. 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:45:16.385405Z",
     "start_time": "2020-08-10T00:45:16.359420Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TVbfyAczjGyH",
    "outputId": "4befe0ea-489f-4158-d7ce-6eb818e021ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/IMDB/labeledTrainData.tsv\"\n",
    "\n",
    "embedding_size = 256 #嵌入词向量的维度\n",
    "kernel_size = [3,4,5] #卷积尺寸\n",
    "num_filters = 100 #每个卷积的数目，即输出的通道数\n",
    "num_classes = 2 #类别数\n",
    "\n",
    "hidden_size = 512  # rnn的隐状态单元维度\n",
    "dropout_rate = 0.2  # RNN的dropout参数\n",
    "num_layers = 2  # rnn层数\n",
    "\n",
    "\n",
    "batch_size = 256 #加载数据的批量大小\n",
    "shuffle = True #加载数据时是否打乱\n",
    "validation_split = 0.2 # 划分验证集的比例\n",
    "num_workers = 0 # 处理器个数\n",
    "max_text_len = 200 #将每条文本处理成相同的最大长度\n",
    "vocab_size = 5000 # 取词频前5000的词\n",
    "\n",
    "epochs = 1000 # 训练次数\n",
    "lr = 0.1 # 学习率\n",
    "do_validation = True # 是否在训练的时候，做验证\n",
    "\n",
    "device = torch.device((\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_ZAid_AjGyQ"
   },
   "source": [
    "## 3. 数据预处理，并划分数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:46:26.886505Z",
     "start_time": "2020-08-10T00:45:16.390404Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RcqgzGvBjGyR"
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()  # 去除html标签\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # 去除标点\n",
    "    words = text.lower().split()  # 全部转成小写，然后按空格分词\n",
    "    words = [w for w in words if w not in eng_stopwords]  # 去除停用词\n",
    "    return ' '.join(words)  # 重组成新的句子\n",
    "\n",
    "\n",
    "def get_word2id(sentences, vocab_size):\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    if vocab_size > len(set(word_list)):\n",
    "        vocab = list(set(word_list))\n",
    "    else:\n",
    "        counter = Counter(word_list).most_common(vocab_size-1)\n",
    "        vocab, _ = list(zip(*counter))\n",
    "\n",
    "    word2id = {w: i+1 for i, w in enumerate(vocab)}\n",
    "    word2id['<UNK>'] = 0  # 未知词\n",
    "\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def token2id(text, max_text_len, word2id):\n",
    "    token2id = [\n",
    "        word2id[w] if w in word2id else word2id['<UNK>'] for w in text.split()\n",
    "    ]\n",
    "\n",
    "    if len(token2id) >= max_text_len:\n",
    "        token2id = token2id[:max_text_len]\n",
    "    else:\n",
    "        token2id = token2id + [word2id['<UNK>']] * \\\n",
    "            (max_text_len - len(token2id))\n",
    "\n",
    "    return token2id\n",
    "\n",
    "\n",
    "def valid_split(data, label, split):\n",
    "    n_samples = data.shape[0]\n",
    "\n",
    "    idx_full = np.arange(n_samples)\n",
    "    np.random.shuffle(idx_full)\n",
    "    \n",
    "    if isinstance(split, int):\n",
    "        assert split > 0\n",
    "        assert split < n_samples\n",
    "        len_valid = split\n",
    "    else:\n",
    "        len_valid = int(n_samples * split)\n",
    "\n",
    "    valid_idx = idx_full[:len_valid]\n",
    "    train_idx = idx_full[len_valid:]\n",
    "\n",
    "    train_data = data[train_idx]\n",
    "    train_label = label[train_idx]\n",
    "\n",
    "    valid_data = data[valid_idx]\n",
    "    valid_label = label[valid_idx]\n",
    "\n",
    "    return train_data, train_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t', escapechar='\\\\')\n",
    "df['clean_review'] = df['review'].apply(text_clean)\n",
    "word2id = get_word2id(df['clean_review'].tolist(), vocab_size)\n",
    "\n",
    "data = df['clean_review'].apply(token2id, args=(max_text_len, word2id))\n",
    "data = data.values\n",
    "label = df['sentiment'].values\n",
    "\n",
    "train_data, train_label, valid_data, valid_label = valid_split(data,label,split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T03:03:22.141952Z",
     "start_time": "2020-08-09T03:03:22.134956Z"
    },
    "colab_type": "text",
    "id": "ElVv0gzzjGye"
   },
   "source": [
    "## 3. 定义Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:46:26.918489Z",
     "start_time": "2020-08-10T00:46:26.892504Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SKZ0hDf9jGyg"
   },
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.data[index]), torch.tensor(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "train_dataset = ImdbDataset(train_data, train_label)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "valid_dataset = ImdbDataset(valid_data, valid_label)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZ2bgRjOjGyz"
   },
   "source": [
    "## 4. 定义模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:46:27.030423Z",
     "start_time": "2020-08-10T00:46:26.922487Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "HyBJ3sLkjGy0",
    "outputId": "24f1115e-1fb2-4418-aa89-ba3135639d9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextRNN(\n",
       "  (embedding): Embedding(5000, 256)\n",
       "  (rnn): RNN(256, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 max_text_len,\n",
    "                 kernel_size,\n",
    "                 num_filters,\n",
    "                 num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                # [batch_size, num_filters, max_text_len-h+1]\n",
    "                nn.Conv1d(in_channels=embedding_size,\n",
    "                          out_channels=num_filters, kernel_size=h),\n",
    "                nn.BatchNorm1d(num_features=num_filters),\n",
    "                nn.ReLU(),\n",
    "                # [batch_size, num_filters*1]\n",
    "                nn.MaxPool1d(kernel_size=max_text_len-h+1)\n",
    "            )\n",
    "            for h in kernel_size\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(in_features=num_filters *\n",
    "                            len(kernel_size), out_features=num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # 分类\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, max_text_len]\n",
    "        embed_x = self.embedding(x)  # [batch_size,max_text_len,embedding_size]\n",
    "\n",
    "        # [batch_size,embedding_size,max_text_len]\n",
    "        embed_x = embed_x.permute(0, 2, 1)\n",
    "\n",
    "        # out[i]: [batch_size, num_filters*1】\n",
    "        out = [conv(embed_x) for conv in self.conv]\n",
    "\n",
    "        # 拼接不同尺寸的卷积核运算出来的结果\n",
    "        # [batch_size, num_filters * len(filter_size)]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.view(-1, out.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.sm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, dropout_rate, num_classes):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        self.fc = self.fc = nn.Linear(\n",
    "            in_features=hidden_size * 2,\n",
    "            out_features=num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)  # [batch_size,max_text_len,embedding_size]\n",
    "        out, _ = self.rnn(out)  # [batch_size, max_text_len, hidden_size*2]\n",
    "        out = self.fc(out[:, -1, :])  # [batch_size,max_text_len,num_classes]\n",
    "        out = self.softmax(out)  # [batch_size, num_classess]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = TextCNN(vocab_size, embedding_size, max_text_len,\n",
    "#                 kernel_size, num_filters, num_classes)\n",
    "\n",
    "model = TextRNN(vocab_size, embedding_size, hidden_size,\n",
    "                num_layers, dropout_rate, num_classes)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taG3AflljGy-"
   },
   "source": [
    "## 5.定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:46:27.062406Z",
     "start_time": "2020-08-10T00:46:27.034421Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rvm3jW3DjGy_"
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, train_dataloader, valid_dataloader, do_validation, device, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 20 == 0:\n",
    "                print('Train Epoch:{}[{}/{}({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                    100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        if do_validation:\n",
    "            model.eval()\n",
    "            valid_loss = 0.0\n",
    "            valid_correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(valid_dataloader):\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output,target)\n",
    "                    valid_loss += loss.item()\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    valid_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            valid_loss /= len(valid_dataloader)\n",
    "\n",
    "            print('\\nValid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                valid_loss, valid_correct, len(valid_dataloader.dataset),\n",
    "                100. * valid_correct / len(valid_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Zr53tuhjGzO"
   },
   "source": [
    "##  6.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T00:53:06.329804Z",
     "start_time": "2020-08-10T00:46:27.066404Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hyDerhcCjGzQ",
    "outputId": "31a8b520-6b55-4a2f-dd5b-398146af32e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0[0/20000(0%)]\tLoss:0.699335\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5e699d0d7e5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m train(epochs, model, train_dataloader,\n\u001b[1;32m----> 2\u001b[1;33m       valid_dataloader, do_validation, device, lr)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-98c12b0042da>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, model, train_dataloader, valid_dataloader, do_validation, device, lr)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m20\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs, model, train_dataloader,\n",
    "      valid_dataloader, do_validation, device, lr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "基于深度学习的文本分类.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python36764bitpytorchcondaaece1924a77c4f7c8ce73fb853bfcc00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 517,
   "position": {
    "height": "539px",
    "left": "989px",
    "right": "20px",
    "top": "151px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

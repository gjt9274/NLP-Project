{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "680FN7pijGx3"
   },
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T04:39:16.301360Z",
     "start_time": "2020-08-09T04:39:16.277376Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "30n6pfOEjGx5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dn7U_kimjGyF"
   },
   "source": [
    "## 2. 定义参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T04:47:17.208443Z",
     "start_time": "2020-08-09T04:47:17.188478Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TVbfyAczjGyH",
    "outputId": "4befe0ea-489f-4158-d7ce-6eb818e021ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/content/labeledTrainData.tsv\"\n",
    "\n",
    "embedding_size = 256 #嵌入词向量的维度\n",
    "kernel_size = [3,4,5] #卷积尺寸\n",
    "num_filters = 100 #每个卷积的数目，即输出的通道数\n",
    "num_classes = 2 #类别数\n",
    "\n",
    "hidden_size = 512  # rnn的隐状态单元维度\n",
    "dropout_rate = 0.2  # RNN的dropout参数\n",
    "num_layers = 2  # rnn层数\n",
    "\n",
    "\n",
    "batch_size = 256 #加载数据的批量大小\n",
    "shuffle = True #加载数据时是否打乱\n",
    "validation_split = 0.2 # 划分验证集的比例\n",
    "num_workers = 1 # 处理器个数\n",
    "max_text_len = 200 #将每条文本处理成相同的最大长度\n",
    "vocab_size = 5000 # 取词频前5000的词\n",
    "\n",
    "epochs = 1000 # 训练次数\n",
    "lr = 0.1 # 学习率\n",
    "do_validation = True # 是否在训练的时候，做验证\n",
    "\n",
    "device = torch.device((\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_ZAid_AjGyQ"
   },
   "source": [
    "## 3. 数据预处理，并划分数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-09T04:47:19.408Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RcqgzGvBjGyR"
   },
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    eng_stopwords = stopwords.words('english')\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()  # 去除html标签\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # 去除标点\n",
    "    words = text.lower().split()  # 全部转成小写，然后按空格分词\n",
    "    words = [w for w in words if w not in eng_stopwords]  # 去除停用词\n",
    "    return ' '.join(words)  # 重组成新的句子\n",
    "\n",
    "\n",
    "def get_word2id(sentences, vocab_size):\n",
    "    word_list = \" \".join(sentences).split()\n",
    "    if vocab_size > len(set(word_list)):\n",
    "        vocab = list(set(word_list))\n",
    "    else:\n",
    "        counter = Counter(word_list).most_common(vocab_size-1)\n",
    "        vocab, _ = list(zip(*counter))\n",
    "\n",
    "    word2id = {w: i+1 for i, w in enumerate(vocab)}\n",
    "    word2id['<UNK>'] = 0  # 未知词\n",
    "\n",
    "    return word2id\n",
    "\n",
    "\n",
    "def token2id(text, max_text_len, word2id):\n",
    "    token2id = [\n",
    "        word2id[w] if w in word2id else word2id['<UNK>'] for w in text.split()\n",
    "    ]\n",
    "\n",
    "    if len(token2id) >= max_text_len:\n",
    "        token2id = token2id[:max_text_len]\n",
    "    else:\n",
    "        token2id = token2id + [word2id['<UNK>']] * \\\n",
    "            (max_text_len - len(token2id))\n",
    "\n",
    "    return token2id\n",
    "\n",
    "\n",
    "def valid_split(data, label, split):\n",
    "    n_samples = data.shape[0]\n",
    "\n",
    "    idx_full = np.arange(n_samples)\n",
    "    np.random.shuffle(idx_full)\n",
    "    \n",
    "    if isinstance(split, int):\n",
    "        assert split > 0\n",
    "        assert split < n_samples\n",
    "        len_valid = split\n",
    "    else:\n",
    "        len_valid = int(n_samples * split)\n",
    "\n",
    "    valid_idx = idx_full[:len_valid]\n",
    "    train_idx = idx_full[len_valid:]\n",
    "\n",
    "    train_data = data[train_idx]\n",
    "    train_label = label[train_idx]\n",
    "\n",
    "    valid_data = data[valid_idx]\n",
    "    valid_label = label[valid_idx]\n",
    "\n",
    "    return train_data, train_label, valid_data, valid_label\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t', escapechar='\\\\')\n",
    "df['clean_review'] = df['review'].apply(text_clean)\n",
    "word2id = get_word2id(df['clean_review'].tolist(), vocab_size)\n",
    "\n",
    "data = df['clean_review'].apply(token2id, args=(max_text_len, word2id))\n",
    "data = data.values\n",
    "label = df['sentiment'].values\n",
    "\n",
    "train_data, train_label, valid_data, valid_label = valid_split(data,label,split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T03:03:22.141952Z",
     "start_time": "2020-08-09T03:03:22.134956Z"
    },
    "colab_type": "text",
    "id": "ElVv0gzzjGye"
   },
   "source": [
    "## 3. 定义Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-09T04:47:22.592Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SKZ0hDf9jGyg"
   },
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.LongTensor(self.data[index]), torch.tensor(self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "train_dataset = ImdbDataset(train_data, train_label)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "valid_dataset = ImdbDataset(valid_data, valid_label)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, batch_size=batch_size, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZ2bgRjOjGyz"
   },
   "source": [
    "## 4. 定义模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T04:12:13.319586Z",
     "start_time": "2020-08-09T04:12:13.246633Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "HyBJ3sLkjGy0",
    "outputId": "24f1115e-1fb2-4418-aa89-ba3135639d9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextRNN(\n",
       "  (embedding): Embedding(5000, 256)\n",
       "  (rnn): RNN(256, 512, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embedding_size,\n",
    "                 max_text_len,\n",
    "                 kernel_size,\n",
    "                 num_filters,\n",
    "                 num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                # [batch_size, num_filters, max_text_len-h+1]\n",
    "                nn.Conv1d(in_channels=embedding_size,\n",
    "                          out_channels=num_filters, kernel_size=h),\n",
    "                nn.BatchNorm1d(num_features=num_filters),\n",
    "                nn.ReLU(),\n",
    "                # [batch_size, num_filters*1]\n",
    "                nn.MaxPool1d(kernel_size=max_text_len-h+1)\n",
    "            )\n",
    "            for h in kernel_size\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(in_features=num_filters *\n",
    "                            len(kernel_size), out_features=num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # 分类\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, max_text_len]\n",
    "        embed_x = self.embedding(x)  # [batch_size,max_text_len,embedding_size]\n",
    "\n",
    "        # [batch_size,embedding_size,max_text_len]\n",
    "        embed_x = embed_x.permute(0, 2, 1)\n",
    "\n",
    "        # out[i]: [batch_size, num_filters*1】\n",
    "        out = [conv(embed_x) for conv in self.conv]\n",
    "\n",
    "        # 拼接不同尺寸的卷积核运算出来的结果\n",
    "        # [batch_size, num_filters * len(filter_size)]\n",
    "        out = torch.cat(out, dim=1)\n",
    "        out = out.view(-1, out.shape[1])\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        out = self.sm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers, dropout_rate, num_classes):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True,\n",
    "            bidirectional=True)\n",
    "        self.fc = self.fc = nn.Linear(\n",
    "            in_features=hidden_size * 2,\n",
    "            out_features=num_classes)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)  # [batch_size,max_text_len,embedding_size]\n",
    "        out, _ = self.rnn(out)  # [batch_size, max_text_len, hidden_size*2]\n",
    "        out = self.fc(out[:, -1, :])  # [batch_size,max_text_len,num_classes]\n",
    "        out = self.softmax(out)  # [batch_size, num_classess]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# model = TextCNN(vocab_size, embedding_size, max_text_len,\n",
    "#                 kernel_size, num_filters, num_classes)\n",
    "\n",
    "model = TextRNN(vocab_size, embedding_size, hidden_size,\n",
    "                num_layers, dropout_rate, num_classes)\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taG3AflljGy-"
   },
   "source": [
    "## 5.定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T04:44:19.250583Z",
     "start_time": "2020-08-09T04:44:19.207608Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rvm3jW3DjGy_"
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, train_dataloader, valid_dataloader, do_validation, device, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            print('Train Epoch:{}[{}/{}({:.0f}%)]\\tLoss:{:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        if do_validation:\n",
    "            model.eval()\n",
    "            valid_loss = 0.0\n",
    "            valid_correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (data, target) in enumerate(valid_dataloader):\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output,target)\n",
    "                    valid_loss += loss.item()\n",
    "                    pred = output.argmax(dim=1, keepdim=True)\n",
    "                    valid_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            valid_loss /= len(valid_dataloader)\n",
    "\n",
    "            print('\\nValid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                valid_loss, valid_correct, len(valid_dataloader.dataset),\n",
    "                100. * valid_correct / len(valid_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Zr53tuhjGzO"
   },
   "source": [
    "##  6.训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T04:14:13.106153Z",
     "start_time": "2020-08-09T04:14:08.964364Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hyDerhcCjGzQ",
    "outputId": "31a8b520-6b55-4a2f-dd5b-398146af32e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:0[0/20000(0%)]\tLoss:0.694363\n",
      "Train Epoch:0[256/20000(1%)]\tLoss:0.853034\n",
      "Train Epoch:0[512/20000(3%)]\tLoss:0.793731\n",
      "Train Epoch:0[768/20000(4%)]\tLoss:0.837872\n",
      "Train Epoch:0[1024/20000(5%)]\tLoss:0.834837\n",
      "Train Epoch:0[1280/20000(6%)]\tLoss:0.828887\n",
      "Train Epoch:0[1536/20000(8%)]\tLoss:0.797646\n",
      "Train Epoch:0[1792/20000(9%)]\tLoss:0.770293\n",
      "Train Epoch:0[2048/20000(10%)]\tLoss:0.782012\n",
      "Train Epoch:0[2304/20000(11%)]\tLoss:0.797638\n",
      "Train Epoch:0[2560/20000(13%)]\tLoss:0.883574\n",
      "Train Epoch:0[2816/20000(14%)]\tLoss:0.782016\n",
      "Train Epoch:0[3072/20000(15%)]\tLoss:0.785918\n",
      "Train Epoch:0[3328/20000(16%)]\tLoss:0.832793\n",
      "Train Epoch:0[3584/20000(18%)]\tLoss:0.832793\n",
      "Train Epoch:0[3840/20000(19%)]\tLoss:0.785918\n",
      "Train Epoch:0[4096/20000(20%)]\tLoss:0.824977\n",
      "Train Epoch:0[4352/20000(22%)]\tLoss:0.818018\n",
      "Train Epoch:0[4608/20000(23%)]\tLoss:0.871855\n",
      "Train Epoch:0[4864/20000(24%)]\tLoss:0.782260\n",
      "Train Epoch:0[5120/20000(25%)]\tLoss:0.793730\n",
      "Train Epoch:0[5376/20000(27%)]\tLoss:0.793730\n",
      "Train Epoch:0[5632/20000(28%)]\tLoss:0.742949\n",
      "Train Epoch:0[5888/20000(29%)]\tLoss:0.770293\n",
      "Train Epoch:0[6144/20000(30%)]\tLoss:0.793730\n",
      "Train Epoch:0[6400/20000(32%)]\tLoss:0.840605\n",
      "Train Epoch:0[6656/20000(33%)]\tLoss:0.805449\n",
      "Train Epoch:0[6912/20000(34%)]\tLoss:0.785918\n",
      "Train Epoch:0[7168/20000(35%)]\tLoss:0.801543\n",
      "Train Epoch:0[7424/20000(37%)]\tLoss:0.770293\n",
      "Train Epoch:0[7680/20000(38%)]\tLoss:0.817168\n",
      "Train Epoch:0[7936/20000(39%)]\tLoss:0.805449\n",
      "Train Epoch:0[8192/20000(41%)]\tLoss:0.856230\n",
      "Train Epoch:0[8448/20000(42%)]\tLoss:0.867949\n",
      "Train Epoch:0[8704/20000(43%)]\tLoss:0.848394\n",
      "Train Epoch:0[8960/20000(44%)]\tLoss:0.801330\n",
      "Train Epoch:0[9216/20000(46%)]\tLoss:0.809721\n",
      "Train Epoch:0[9472/20000(47%)]\tLoss:0.828886\n",
      "Train Epoch:0[9728/20000(48%)]\tLoss:0.817167\n",
      "Train Epoch:0[9984/20000(49%)]\tLoss:0.824980\n",
      "Train Epoch:0[10240/20000(51%)]\tLoss:0.821074\n",
      "Train Epoch:0[10496/20000(52%)]\tLoss:0.832793\n",
      "Train Epoch:0[10752/20000(53%)]\tLoss:0.817168\n",
      "Train Epoch:0[11008/20000(54%)]\tLoss:0.848418\n",
      "Train Epoch:0[11264/20000(56%)]\tLoss:0.762480\n",
      "Train Epoch:0[11520/20000(57%)]\tLoss:0.785918\n",
      "Train Epoch:0[11776/20000(58%)]\tLoss:0.828885\n",
      "Train Epoch:0[12032/20000(59%)]\tLoss:0.797637\n",
      "Train Epoch:0[12288/20000(61%)]\tLoss:0.813131\n",
      "Train Epoch:0[12544/20000(62%)]\tLoss:0.864043\n",
      "Train Epoch:0[12800/20000(63%)]\tLoss:0.821357\n",
      "Train Epoch:0[13056/20000(65%)]\tLoss:0.770293\n",
      "Train Epoch:0[13312/20000(66%)]\tLoss:0.848418\n",
      "Train Epoch:0[13568/20000(67%)]\tLoss:0.805449\n",
      "Train Epoch:0[13824/20000(68%)]\tLoss:0.801543\n",
      "Train Epoch:0[14080/20000(70%)]\tLoss:0.836699\n",
      "Train Epoch:0[14336/20000(71%)]\tLoss:0.742949\n",
      "Train Epoch:0[14592/20000(72%)]\tLoss:0.813262\n",
      "Train Epoch:0[14848/20000(73%)]\tLoss:0.785917\n",
      "Train Epoch:0[15104/20000(75%)]\tLoss:0.821074\n",
      "Train Epoch:0[15360/20000(76%)]\tLoss:0.860136\n",
      "Train Epoch:0[15616/20000(77%)]\tLoss:0.840605\n",
      "Train Epoch:0[15872/20000(78%)]\tLoss:0.782022\n",
      "Train Epoch:0[16128/20000(80%)]\tLoss:0.844472\n",
      "Train Epoch:0[16384/20000(81%)]\tLoss:0.797637\n",
      "Train Epoch:0[16640/20000(82%)]\tLoss:0.774199\n",
      "Train Epoch:0[16896/20000(84%)]\tLoss:0.805449\n",
      "Train Epoch:0[17152/20000(85%)]\tLoss:0.785918\n",
      "Train Epoch:0[17408/20000(86%)]\tLoss:0.821074\n",
      "Train Epoch:0[17664/20000(87%)]\tLoss:0.805446\n",
      "Train Epoch:0[17920/20000(89%)]\tLoss:0.797649\n",
      "Train Epoch:0[18176/20000(90%)]\tLoss:0.848556\n",
      "Train Epoch:0[18432/20000(91%)]\tLoss:0.805449\n",
      "Train Epoch:0[18688/20000(92%)]\tLoss:0.809355\n",
      "Train Epoch:0[18944/20000(94%)]\tLoss:0.817168\n",
      "Train Epoch:0[19200/20000(95%)]\tLoss:0.832793\n",
      "Train Epoch:0[19456/20000(96%)]\tLoss:0.864043\n",
      "Train Epoch:0[19712/20000(97%)]\tLoss:0.813262\n",
      "Train Epoch:0[2496/20000(99%)]\tLoss:0.750762\n",
      "\n",
      "Valid set: Average loss: 0.8181, Accuracy: 2469/5000 (49%)\n",
      "\n",
      "Train Epoch:1[0/20000(0%)]\tLoss:0.824980\n",
      "Train Epoch:1[256/20000(1%)]\tLoss:0.832793\n",
      "Train Epoch:1[512/20000(3%)]\tLoss:0.770197\n",
      "Train Epoch:1[768/20000(4%)]\tLoss:0.871861\n",
      "Train Epoch:1[1024/20000(5%)]\tLoss:0.828886\n",
      "Train Epoch:1[1280/20000(6%)]\tLoss:0.824980\n",
      "Train Epoch:1[1536/20000(8%)]\tLoss:0.797635\n",
      "Train Epoch:1[1792/20000(9%)]\tLoss:0.766387\n",
      "Train Epoch:1[2048/20000(10%)]\tLoss:0.782012\n",
      "Train Epoch:1[2304/20000(11%)]\tLoss:0.793730\n",
      "Train Epoch:1[2560/20000(13%)]\tLoss:0.879668\n",
      "Train Epoch:1[2816/20000(14%)]\tLoss:0.782012\n",
      "Train Epoch:1[3072/20000(15%)]\tLoss:0.778105\n",
      "Train Epoch:1[3328/20000(16%)]\tLoss:0.836699\n",
      "Train Epoch:1[3584/20000(18%)]\tLoss:0.852324\n",
      "Train Epoch:1[3840/20000(19%)]\tLoss:0.782012\n",
      "Train Epoch:1[4096/20000(20%)]\tLoss:0.820956\n",
      "Train Epoch:1[4352/20000(22%)]\tLoss:0.790514\n",
      "Train Epoch:1[4608/20000(23%)]\tLoss:0.880193\n",
      "Train Epoch:1[4864/20000(24%)]\tLoss:0.774199\n",
      "Train Epoch:1[5120/20000(25%)]\tLoss:0.775210\n",
      "Train Epoch:1[5376/20000(27%)]\tLoss:0.829683\n",
      "Train Epoch:1[5632/20000(28%)]\tLoss:0.875811\n",
      "Train Epoch:1[5888/20000(29%)]\tLoss:0.862224\n",
      "Train Epoch:1[6144/20000(30%)]\tLoss:0.836699\n",
      "Train Epoch:1[6400/20000(32%)]\tLoss:0.782012\n",
      "Train Epoch:1[6656/20000(33%)]\tLoss:0.832793\n",
      "Train Epoch:1[6912/20000(34%)]\tLoss:0.848418\n",
      "Train Epoch:1[7168/20000(35%)]\tLoss:0.832793\n"
     ]
    }
   ],
   "source": [
    "train(epochs, model, train_dataloader,\n",
    "      valid_dataloader, do_validation, device, lr)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "基于深度学习的文本分类.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.7 64-bit ('pytorch': conda)",
   "language": "python",
   "name": "python36764bitpytorchcondaaece1924a77c4f7c8ce73fb853bfcc00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 517,
   "position": {
    "height": "539px",
    "left": "989px",
    "right": "20px",
    "top": "151px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T02:12:11.727952Z",
     "start_time": "2020-08-03T02:12:11.709965Z"
    }
   },
   "source": [
    "## 1. 任务介绍\n",
    "> 介绍任务的基本内容，以及问题的公式化\n",
    "---\n",
    "文本分类是自然语言处理中最基础的任务之一，主要是通过分类器将给定的文本划分到特定的类，比如情绪分类、垃圾邮件分类、电影评论分类等。具体任务公式化如下：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "文本 ： &X = (x_1,x_2,\\dots,x_n) \\\\\n",
    "类标签 ：& Y = (y_1,y_2,\\dots,y_n)\\\\\n",
    "模型 ：& f: x_i  \\xrightarrow{f} y_i, \\hspace{1em} i = 1,2, \\dots,n\n",
    "\\end{aligned}\n",
    "$$\n",
    "本文选用Kaggle的电影评论情感分析来作为任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T02:54:37.248967Z",
     "start_time": "2020-08-05T02:54:33.892925Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "N_CLASSES = 2 # 类别数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据预处理\n",
    "> 需要对数据进行清洗\n",
    "---\n",
    "\n",
    "处理步骤大致如下：\n",
    "    1. 去除html标签\n",
    "    2. 去除标点\n",
    "    3. 切分成词\n",
    "    4. 去除停用词\n",
    "    5. 重组为新的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T02:54:37.738100Z",
     "start_time": "2020-08-05T02:54:37.252956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:25000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0. 先准备数据\n",
    "file_path = '../data/IMDB/labeledTrainData.tsv'\n",
    "df = pd.read_csv(file_path,sep='\\t',escapechar='\\\\')\n",
    "print('Number of samples:{}'.format(len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T02:55:05.580700Z",
     "start_time": "2020-08-05T02:54:37.742098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff going moment mj started listening music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The Classic War of the Worlds\" by Timothy Hin...</td>\n",
       "      <td>classic war worlds timothy hines entertaining ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film starts manager nicholas bell giving welco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>must assumed praised film greatest filmed oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious explo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \"The Classic War of the Worlds\" by Timothy Hin...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  stuff going moment mj started listening music ...  \n",
       "1  classic war worlds timothy hines entertaining ...  \n",
       "2  film starts manager nicholas bell giving welco...  \n",
       "3  must assumed praised film greatest filmed oper...  \n",
       "4  superbly trashy wondrously unpretentious explo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_stopwords = stopwords.words('english') #定义停用词\n",
    "\n",
    "def text_clean(text):\n",
    "    text = BeautifulSoup(text,'html.parser').get_text() #去除html标签\n",
    "    text = re.sub(r'[^a-zA-Z]',' ',text) #去除标点\n",
    "    words = text.lower().split()  #全部转成小写，然后按空格分词\n",
    "    words = [w for w in words if w not in eng_stopwords] #去除停用词\n",
    "    return ' '.join(words)  #重组成新的句子\n",
    "\n",
    "df['clean_review'] = df.review.apply(text_clean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 文本特征表示 \n",
    "> 文本向量化，并提取特征，分为离散法和分布式法(此处主要讲离散法)\n",
    "---\n",
    "离散法主要有以下几种方法：\n",
    " 1. 词袋模型(Bag of word): 用单词频数来表示文本，不考虑文本的语法结构和单词顺序\n",
    " 2. 独热编码(One-hot): 将文本表示成整个词标长度的向量，出现过的词为1，否则为0\n",
    " 3. n元语法(n-gram): 对词袋模型的一种改进，即用n个词或词组组成的字符串作为特征，然后再用词袋模型的方法将文本表示为向量形式\n",
    " 4. TF-IDF： 用词的TF-IDF来表示文本\n",
    " \n",
    "<span style=\"color:red\">注：其实上述方法都可以看作是词袋模型</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T02:55:38.100109Z",
     "start_time": "2020-08-05T02:55:05.586699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词频为特征的文本-单词矩阵维度: (25000, 5000)\n",
      "bi-gram为特征的文本-单词矩阵维度： (25000, 1000)\n",
      "TF-IDF为特征的文本-单词矩阵维度： (25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# 1. 使用统计词频，作为文本特征\n",
    "vectorizer_feq = CountVectorizer(max_features=5000) #取词频为前5000的词\n",
    "data_freq = vectorizer_feq.fit_transform(df.clean_review).toarray()\n",
    "print(\"词频为特征的文本-单词矩阵维度:\",data_freq.shape)\n",
    "\n",
    "# 2. 使用bigram，作为文本特征\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2,2),max_features=1000,token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "# analyze = vectorizer_bigram.build_analyzer()\n",
    "# print(\"bi-gram示例：\",analyze(df.clean_review[0]))#bi-gram举例\n",
    "data_bigram = vectorizer_bigram.fit_transform(df.clean_review).toarray()\n",
    "print(\"bi-gram为特征的文本-单词矩阵维度：\",data_bigram.shape)\n",
    "\n",
    "# 2. 使用tfidf, 作为文本特征\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "data_tfidf = vectorizer_tfidf.fit_transform(df.clean_review).toarray()\n",
    "\n",
    "print(\"TF-IDF为特征的文本-单词矩阵维度：\",data_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 辅助函数\n",
    "> 包括数据批量生成器，softmax函数，预测函数，评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T02:55:38.179065Z",
     "start_time": "2020-08-05T02:55:38.105108Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义数据批量生成器\n",
    "def batch_generator(data, batch_size, shuffle=True):\n",
    "    X, Y = data\n",
    "    n_samples = X.shape[0]\n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)  # 打乱顺序\n",
    "\n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_idx = indices[start:end]\n",
    "\n",
    "        yield X[batch_idx], Y[batch_idx]\n",
    "\n",
    "\n",
    "# softmax函数\n",
    "def softmax(scores):\n",
    "    sum_exp = np.sum(np.exp(scores), axis=1, keepdims=True)\n",
    "    softmax = np.exp(scores) / sum_exp\n",
    "    return softmax\n",
    "\n",
    "# 预测函数\n",
    "def predict(w,x):\n",
    "    scores = np.dot(x, w.T)\n",
    "    probs = softmax(scores)\n",
    "\n",
    "    return np.argmax(probs, axis=1).reshape(-1, 1)\n",
    "\n",
    "# 评估函数\n",
    "def evaluate(y_true,y_pred):\n",
    "    precision = metrics.precision_score(y_true,y_pred)\n",
    "    recall = metrics.recall_score(y_true,y_pred)\n",
    "    f1_score = metrics.f1_score(y_true,y_pred)\n",
    "\n",
    "    return precision,recall,f1_score\n",
    "\n",
    "\n",
    "# 将数据集分割\n",
    "def split_data(x, y, val_split=0.2):\n",
    "    n_samples = x.shape[0]\n",
    "\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    split = int(n_samples * (1 - val_split))\n",
    "    training_idx = indices[:split]\n",
    "    valid_idx = indices[split:]\n",
    "\n",
    "    train_x = x[training_idx]\n",
    "    train_y = y[training_idx]\n",
    "\n",
    "    valid_x = x[valid_idx]\n",
    "    valid_y = y[valid_idx]\n",
    "\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 构建分类器\n",
    "> 此处以softmax regression作为分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T02:55:38.243031Z",
     "start_time": "2020-08-05T02:55:38.184063Z"
    }
   },
   "outputs": [],
   "source": [
    "def valid(w, val_x, val_y, batch_size):\n",
    "    val_loss = []\n",
    "    val_gen = batch_generator((val_x, val_y), batch_size, shuffle=False)\n",
    "\n",
    "    for batch_x, batch_y in val_gen:\n",
    "        scores = np.dot(batch_x, w.T)\n",
    "        prob = softmax(scores)\n",
    "\n",
    "        y_one_hot = np.eye(N_CLASSES)[batch_y]\n",
    "        # 损失函数\n",
    "        loss = -(1.0 / len(batch_x)) * np.sum(y_one_hot * np.log(prob))\n",
    "        val_loss.append(loss)\n",
    "\n",
    "    return np.mean(val_loss)\n",
    "\n",
    "\n",
    "def train(train_x,\n",
    "          train_y,\n",
    "          valid_x,\n",
    "          valid_y,\n",
    "          lr=0.1,\n",
    "          batch_size=128,\n",
    "          epochs=5000,\n",
    "          early_stop=None):\n",
    "\n",
    "    n_features = train_x.shape[1]\n",
    "    w = np.random.rand(N_CLASSES, n_features)\n",
    "\n",
    "    train_all_loss = []\n",
    "    val_all_loss = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    f1_score_list = []\n",
    "\n",
    "    not_improved = 0\n",
    "    best_val_loss = np.inf\n",
    "    best_w = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        training_gen = batch_generator((train_x, train_y), batch_size)\n",
    "        train_loss = []\n",
    "        for batch_x, batch_y in training_gen:\n",
    "            scores = np.dot(batch_x, w.T)\n",
    "            prob = softmax(scores)\n",
    "\n",
    "            y_one_hot = np.eye(N_CLASSES)[batch_y]\n",
    "            # 损失函数\n",
    "            loss = -(1.0 / len(batch_x)) * np.sum(y_one_hot * np.log(prob))\n",
    "            train_loss.append(loss)\n",
    "\n",
    "            # 梯度下降\n",
    "            dw = -(1.0 / len(batch_x)) * np.dot((y_one_hot - prob).T, batch_x)\n",
    "            w = w - lr * dw\n",
    "            \n",
    "        val_loss = valid(w, valid_x, valid_y, batch_size)\n",
    "\n",
    "\n",
    "        val_precision, val_recall, val_f1_score = evaluate(\n",
    "            valid_y, predict(w, valid_x))\n",
    "\n",
    "        print(\n",
    "            \"Epoch = {0},the train loss = {1:.4f}, the val loss = {2:.4f}, precision={3:.4f}%, recall={4:.4f}%, f1_score={4:.4f}%\"\n",
    "            .format(epoch, np.mean(train_loss), val_loss, val_precision * 100,\n",
    "                    val_recall * 100, val_f1_score * 100))\n",
    "\n",
    "        train_all_loss.append(np.mean(train_loss))\n",
    "        val_all_loss.append(val_loss)\n",
    "        precision_list.append(val_precision)\n",
    "        recall_list.append(val_recall)\n",
    "        f1_score_list.append(val_f1_score)\n",
    "\n",
    "        if early_stop is not None:\n",
    "            if val_loss <= best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_w = w\n",
    "                not_improved = 0\n",
    "            else:\n",
    "                not_improved += 1\n",
    "\n",
    "            if not_improved > early_stop:\n",
    "                print(\"Validation performance didn\\'t improve for {} epochs. \"\n",
    "                      \"Training stops.\".format(early_stop))\n",
    "                break\n",
    "\n",
    "    return best_w, train_all_loss, val_all_loss,precision_list,recall_list,f1_score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.训练 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 以BOW为特征进行训练 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T03:19:27.837570Z",
     "start_time": "2020-08-05T02:55:38.247027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0,the train loss = 0.7187, the val loss = 0.7081, precision=50.2999%, recall=54.1129%, f1_score=54.1129%\n",
      "Epoch = 1,the train loss = 0.7052, the val loss = 0.6959, precision=52.9435%, recall=54.0323%, f1_score=54.0323%\n",
      "Epoch = 2,the train loss = 0.6927, the val loss = 0.6845, precision=55.4871%, recall=54.4355%, f1_score=54.4355%\n",
      "Epoch = 3,the train loss = 0.6809, the val loss = 0.6739, precision=57.7277%, recall=55.7258%, f1_score=55.7258%\n",
      "Epoch = 4,the train loss = 0.6701, the val loss = 0.6638, precision=59.8488%, recall=57.4597%, f1_score=57.4597%\n",
      "Epoch = 5,the train loss = 0.6597, the val loss = 0.6542, precision=61.9514%, recall=58.6290%, f1_score=58.6290%\n",
      "Epoch = 6,the train loss = 0.6498, the val loss = 0.6451, precision=63.3035%, recall=59.9597%, f1_score=59.9597%\n",
      "Epoch = 7,the train loss = 0.6405, the val loss = 0.6364, precision=64.9235%, recall=61.5726%, f1_score=61.5726%\n",
      "Epoch = 8,the train loss = 0.6313, the val loss = 0.6281, precision=66.6947%, recall=63.9516%, f1_score=63.9516%\n",
      "Epoch = 9,the train loss = 0.6232, the val loss = 0.6201, precision=67.7486%, recall=65.6452%, f1_score=65.6452%\n",
      "Epoch = 10,the train loss = 0.6149, the val loss = 0.6126, precision=68.6957%, recall=66.8952%, f1_score=66.8952%\n",
      "Epoch = 11,the train loss = 0.6069, the val loss = 0.6053, precision=69.7070%, recall=68.1048%, f1_score=68.1048%\n",
      "Epoch = 12,the train loss = 0.5996, the val loss = 0.5984, precision=70.8590%, recall=69.5161%, f1_score=69.5161%\n",
      "Epoch = 13,the train loss = 0.5925, the val loss = 0.5917, precision=71.5450%, recall=70.7661%, f1_score=70.7661%\n",
      "Epoch = 14,the train loss = 0.5854, the val loss = 0.5853, precision=72.1841%, recall=72.0968%, f1_score=72.0968%\n",
      "Epoch = 15,the train loss = 0.5789, the val loss = 0.5792, precision=73.0614%, recall=72.9435%, f1_score=72.9435%\n",
      "Epoch = 16,the train loss = 0.5726, the val loss = 0.5733, precision=73.7138%, recall=73.9516%, f1_score=73.9516%\n",
      "Epoch = 17,the train loss = 0.5665, the val loss = 0.5677, precision=74.3292%, recall=74.8387%, f1_score=74.8387%\n",
      "Epoch = 18,the train loss = 0.5605, the val loss = 0.5622, precision=74.6825%, recall=75.8871%, f1_score=75.8871%\n",
      "Epoch = 19,the train loss = 0.5552, the val loss = 0.5570, precision=75.0888%, recall=76.6935%, f1_score=76.6935%\n",
      "Epoch = 20,the train loss = 0.5496, the val loss = 0.5520, precision=75.4836%, recall=77.0968%, f1_score=77.0968%\n",
      "Epoch = 21,the train loss = 0.5445, the val loss = 0.5471, precision=75.6799%, recall=77.4194%, f1_score=77.4194%\n",
      "Epoch = 22,the train loss = 0.5394, the val loss = 0.5424, precision=76.1100%, recall=78.1048%, f1_score=78.1048%\n",
      "Epoch = 23,the train loss = 0.5348, the val loss = 0.5379, precision=76.5306%, recall=78.6290%, f1_score=78.6290%\n",
      "Epoch = 24,the train loss = 0.5297, the val loss = 0.5335, precision=76.9110%, recall=79.1129%, f1_score=79.1129%\n",
      "Epoch = 25,the train loss = 0.5253, the val loss = 0.5293, precision=77.2231%, recall=79.8387%, f1_score=79.8387%\n",
      "Epoch = 26,the train loss = 0.5209, the val loss = 0.5252, precision=77.6352%, recall=80.4839%, f1_score=80.4839%\n",
      "Epoch = 27,the train loss = 0.5167, the val loss = 0.5212, precision=77.9246%, recall=80.8468%, f1_score=80.8468%\n",
      "Epoch = 28,the train loss = 0.5127, the val loss = 0.5174, precision=78.1347%, recall=81.4113%, f1_score=81.4113%\n",
      "Epoch = 29,the train loss = 0.5087, the val loss = 0.5137, precision=78.4967%, recall=81.6935%, f1_score=81.6935%\n",
      "Epoch = 30,the train loss = 0.5045, the val loss = 0.5101, precision=78.6295%, recall=81.8952%, f1_score=81.8952%\n",
      "Epoch = 31,the train loss = 0.5010, the val loss = 0.5066, precision=78.6790%, recall=82.1371%, f1_score=82.1371%\n",
      "Epoch = 32,the train loss = 0.4974, the val loss = 0.5032, precision=78.8417%, recall=82.3387%, f1_score=82.3387%\n",
      "Epoch = 33,the train loss = 0.4938, the val loss = 0.5000, precision=79.1780%, recall=82.3387%, f1_score=82.3387%\n",
      "Epoch = 34,the train loss = 0.4901, the val loss = 0.4968, precision=79.3878%, recall=82.6210%, f1_score=82.6210%\n",
      "Epoch = 35,the train loss = 0.4874, the val loss = 0.4937, precision=79.5903%, recall=83.0242%, f1_score=83.0242%\n",
      "Epoch = 36,the train loss = 0.4837, the val loss = 0.4907, precision=79.8147%, recall=83.3871%, f1_score=83.3871%\n",
      "Epoch = 37,the train loss = 0.4807, the val loss = 0.4878, precision=79.8997%, recall=83.5081%, f1_score=83.5081%\n",
      "Epoch = 38,the train loss = 0.4777, the val loss = 0.4849, precision=79.9769%, recall=83.7500%, f1_score=83.7500%\n",
      "Epoch = 39,the train loss = 0.4746, the val loss = 0.4821, precision=80.1774%, recall=83.8306%, f1_score=83.8306%\n",
      "Epoch = 40,the train loss = 0.4719, the val loss = 0.4795, precision=80.3392%, recall=84.0323%, f1_score=84.0323%\n",
      "Epoch = 41,the train loss = 0.4688, the val loss = 0.4768, precision=80.5320%, recall=84.2339%, f1_score=84.2339%\n",
      "Epoch = 42,the train loss = 0.4658, the val loss = 0.4743, precision=80.6166%, recall=84.3548%, f1_score=84.3548%\n",
      "Epoch = 43,the train loss = 0.4633, the val loss = 0.4718, precision=80.8330%, recall=84.5161%, f1_score=84.5161%\n",
      "Epoch = 44,the train loss = 0.4607, the val loss = 0.4693, precision=80.9487%, recall=84.6371%, f1_score=84.6371%\n",
      "Epoch = 45,the train loss = 0.4584, the val loss = 0.4670, precision=80.9927%, recall=84.8790%, f1_score=84.8790%\n",
      "Epoch = 46,the train loss = 0.4555, the val loss = 0.4646, precision=81.0458%, recall=85.0000%, f1_score=85.0000%\n",
      "Epoch = 47,the train loss = 0.4535, the val loss = 0.4624, precision=81.3246%, recall=85.1613%, f1_score=85.1613%\n",
      "Epoch = 48,the train loss = 0.4512, the val loss = 0.4602, precision=81.3918%, recall=85.3629%, f1_score=85.3629%\n",
      "Epoch = 49,the train loss = 0.4482, the val loss = 0.4580, precision=81.4231%, recall=85.3629%, f1_score=85.3629%\n",
      "Epoch = 50,the train loss = 0.4463, the val loss = 0.4559, precision=81.4929%, recall=85.4032%, f1_score=85.4032%\n",
      "Epoch = 51,the train loss = 0.4441, the val loss = 0.4539, precision=81.4758%, recall=85.4839%, f1_score=85.4839%\n",
      "Epoch = 52,the train loss = 0.4422, the val loss = 0.4518, precision=81.5284%, recall=85.6048%, f1_score=85.6048%\n",
      "Epoch = 53,the train loss = 0.4397, the val loss = 0.4499, precision=81.6123%, recall=85.7258%, f1_score=85.7258%\n",
      "Epoch = 54,the train loss = 0.4376, the val loss = 0.4480, precision=81.7972%, recall=85.8871%, f1_score=85.8871%\n",
      "Epoch = 55,the train loss = 0.4356, the val loss = 0.4461, precision=81.8042%, recall=85.9274%, f1_score=85.9274%\n",
      "Epoch = 56,the train loss = 0.4336, the val loss = 0.4442, precision=81.8740%, recall=85.9677%, f1_score=85.9677%\n",
      "Epoch = 57,the train loss = 0.4316, the val loss = 0.4424, precision=81.9161%, recall=85.8468%, f1_score=85.8468%\n",
      "Epoch = 58,the train loss = 0.4298, the val loss = 0.4407, precision=82.0246%, recall=85.9274%, f1_score=85.9274%\n",
      "Epoch = 59,the train loss = 0.4277, the val loss = 0.4389, precision=82.1195%, recall=85.9274%, f1_score=85.9274%\n",
      "Epoch = 60,the train loss = 0.4258, the val loss = 0.4372, precision=82.1649%, recall=86.0081%, f1_score=86.0081%\n",
      "Epoch = 61,the train loss = 0.4246, the val loss = 0.4356, precision=82.3053%, recall=86.0887%, f1_score=86.0887%\n",
      "Epoch = 62,the train loss = 0.4223, the val loss = 0.4339, precision=82.3507%, recall=86.1694%, f1_score=86.1694%\n",
      "Epoch = 63,the train loss = 0.4208, the val loss = 0.4323, precision=82.4210%, recall=86.2097%, f1_score=86.2097%\n",
      "Epoch = 64,the train loss = 0.4188, the val loss = 0.4308, precision=82.5366%, recall=86.3306%, f1_score=86.3306%\n",
      "Epoch = 65,the train loss = 0.4168, the val loss = 0.4292, precision=82.5685%, recall=86.3306%, f1_score=86.3306%\n",
      "Epoch = 66,the train loss = 0.4155, the val loss = 0.4277, precision=82.5752%, recall=86.3710%, f1_score=86.3710%\n",
      "Epoch = 67,the train loss = 0.4137, the val loss = 0.4262, precision=82.7094%, recall=86.4113%, f1_score=86.4113%\n",
      "Epoch = 68,the train loss = 0.4126, the val loss = 0.4248, precision=82.7733%, recall=86.4113%, f1_score=86.4113%\n",
      "Epoch = 69,the train loss = 0.4106, the val loss = 0.4234, precision=82.8185%, recall=86.4919%, f1_score=86.4919%\n",
      "Epoch = 70,the train loss = 0.4091, the val loss = 0.4220, precision=82.8891%, recall=86.5323%, f1_score=86.5323%\n",
      "Epoch = 71,the train loss = 0.4078, the val loss = 0.4206, precision=82.9598%, recall=86.5726%, f1_score=86.5726%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 72,the train loss = 0.4064, the val loss = 0.4192, precision=83.0371%, recall=86.6532%, f1_score=86.6532%\n",
      "Epoch = 73,the train loss = 0.4049, the val loss = 0.4179, precision=83.1913%, recall=86.6129%, f1_score=86.6129%\n",
      "Epoch = 74,the train loss = 0.4034, the val loss = 0.4166, precision=83.3010%, recall=86.6935%, f1_score=86.6935%\n",
      "Epoch = 75,the train loss = 0.4017, the val loss = 0.4153, precision=83.3527%, recall=86.8145%, f1_score=86.8145%\n",
      "Epoch = 76,the train loss = 0.4007, the val loss = 0.4141, precision=83.3204%, recall=86.8145%, f1_score=86.8145%\n",
      "Epoch = 77,the train loss = 0.3992, the val loss = 0.4128, precision=83.3850%, recall=86.8145%, f1_score=86.8145%\n",
      "Epoch = 78,the train loss = 0.3977, the val loss = 0.4116, precision=83.3914%, recall=86.8548%, f1_score=86.8548%\n",
      "Epoch = 79,the train loss = 0.3963, the val loss = 0.4104, precision=83.3978%, recall=86.8952%, f1_score=86.8952%\n",
      "Epoch = 80,the train loss = 0.3952, the val loss = 0.4092, precision=83.4429%, recall=86.9758%, f1_score=86.9758%\n",
      "Epoch = 81,the train loss = 0.3941, the val loss = 0.4081, precision=83.4429%, recall=86.9758%, f1_score=86.9758%\n",
      "Epoch = 82,the train loss = 0.3925, the val loss = 0.4069, precision=83.4107%, recall=86.9758%, f1_score=86.9758%\n",
      "Epoch = 83,the train loss = 0.3914, the val loss = 0.4058, precision=83.4107%, recall=86.9758%, f1_score=86.9758%\n",
      "Epoch = 84,the train loss = 0.3901, the val loss = 0.4047, precision=83.4107%, recall=86.9758%, f1_score=86.9758%\n",
      "Epoch = 85,the train loss = 0.3891, the val loss = 0.4036, precision=83.4171%, recall=87.0161%, f1_score=87.0161%\n",
      "Epoch = 86,the train loss = 0.3879, the val loss = 0.4025, precision=83.4493%, recall=87.0161%, f1_score=87.0161%\n",
      "Epoch = 87,the train loss = 0.3861, the val loss = 0.4015, precision=83.4880%, recall=87.0565%, f1_score=87.0565%\n",
      "Epoch = 88,the train loss = 0.3856, the val loss = 0.4005, precision=83.4880%, recall=87.0565%, f1_score=87.0565%\n",
      "Epoch = 89,the train loss = 0.3843, the val loss = 0.3994, precision=83.5267%, recall=87.0968%, f1_score=87.0968%\n",
      "Epoch = 90,the train loss = 0.3836, the val loss = 0.3984, precision=83.5590%, recall=87.0968%, f1_score=87.0968%\n",
      "Epoch = 91,the train loss = 0.3826, the val loss = 0.3974, precision=83.6561%, recall=87.0968%, f1_score=87.0968%\n",
      "Epoch = 92,the train loss = 0.3808, the val loss = 0.3965, precision=83.7011%, recall=87.1774%, f1_score=87.1774%\n",
      "Epoch = 93,the train loss = 0.3798, the val loss = 0.3955, precision=83.7597%, recall=87.1371%, f1_score=87.1371%\n",
      "Epoch = 94,the train loss = 0.3788, the val loss = 0.3945, precision=83.7597%, recall=87.1371%, f1_score=87.1371%\n",
      "Epoch = 95,the train loss = 0.3780, the val loss = 0.3936, precision=83.7922%, recall=87.1371%, f1_score=87.1371%\n",
      "Epoch = 96,the train loss = 0.3771, the val loss = 0.3927, precision=83.7922%, recall=87.1371%, f1_score=87.1371%\n",
      "Epoch = 97,the train loss = 0.3761, the val loss = 0.3918, precision=83.8184%, recall=87.0968%, f1_score=87.0968%\n",
      "Epoch = 98,the train loss = 0.3748, the val loss = 0.3909, precision=83.8247%, recall=87.1371%, f1_score=87.1371%\n",
      "Epoch = 99,the train loss = 0.3741, the val loss = 0.3900, precision=83.8309%, recall=87.1774%, f1_score=87.1774%\n",
      "Epoch = 100,the train loss = 0.3729, the val loss = 0.3891, precision=83.9022%, recall=87.2177%, f1_score=87.2177%\n",
      "Epoch = 101,the train loss = 0.3724, the val loss = 0.3882, precision=83.9410%, recall=87.2581%, f1_score=87.2581%\n",
      "Epoch = 102,the train loss = 0.3709, the val loss = 0.3874, precision=84.0186%, recall=87.3387%, f1_score=87.3387%\n",
      "Epoch = 103,the train loss = 0.3696, the val loss = 0.3866, precision=84.0636%, recall=87.4194%, f1_score=87.4194%\n",
      "Epoch = 104,the train loss = 0.3692, the val loss = 0.3857, precision=84.1411%, recall=87.5000%, f1_score=87.5000%\n",
      "Epoch = 105,the train loss = 0.3680, the val loss = 0.3849, precision=84.1411%, recall=87.5000%, f1_score=87.5000%\n",
      "Epoch = 106,the train loss = 0.3672, the val loss = 0.3841, precision=84.1860%, recall=87.5806%, f1_score=87.5806%\n",
      "Epoch = 107,the train loss = 0.3666, the val loss = 0.3833, precision=84.1534%, recall=87.5806%, f1_score=87.5806%\n",
      "Epoch = 108,the train loss = 0.3660, the val loss = 0.3825, precision=84.1534%, recall=87.5806%, f1_score=87.5806%\n",
      "Epoch = 109,the train loss = 0.3646, the val loss = 0.3818, precision=84.1147%, recall=87.5403%, f1_score=87.5403%\n",
      "Epoch = 110,the train loss = 0.3635, the val loss = 0.3810, precision=84.1676%, recall=87.4597%, f1_score=87.4597%\n",
      "Epoch = 111,the train loss = 0.3629, the val loss = 0.3802, precision=84.2841%, recall=87.5806%, f1_score=87.5806%\n",
      "Epoch = 112,the train loss = 0.3623, the val loss = 0.3795, precision=84.3289%, recall=87.6613%, f1_score=87.6613%\n",
      "Epoch = 113,the train loss = 0.3613, the val loss = 0.3787, precision=84.3350%, recall=87.7016%, f1_score=87.7016%\n",
      "Epoch = 114,the train loss = 0.3614, the val loss = 0.3780, precision=84.3411%, recall=87.7419%, f1_score=87.7419%\n",
      "Epoch = 115,the train loss = 0.3600, the val loss = 0.3773, precision=84.3859%, recall=87.8226%, f1_score=87.8226%\n",
      "Epoch = 116,the train loss = 0.3586, the val loss = 0.3766, precision=84.4246%, recall=87.8629%, f1_score=87.8629%\n",
      "Epoch = 117,the train loss = 0.3581, the val loss = 0.3759, precision=84.3980%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 118,the train loss = 0.3574, the val loss = 0.3752, precision=84.3980%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 119,the train loss = 0.3565, the val loss = 0.3745, precision=84.3980%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 120,the train loss = 0.3563, the val loss = 0.3738, precision=84.3653%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 121,the train loss = 0.3550, the val loss = 0.3732, precision=84.3327%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 122,the train loss = 0.3536, the val loss = 0.3725, precision=84.3980%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 123,the train loss = 0.3532, the val loss = 0.3719, precision=84.4307%, recall=87.9032%, f1_score=87.9032%\n",
      "Epoch = 124,the train loss = 0.3522, the val loss = 0.3712, precision=84.4754%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 125,the train loss = 0.3523, the val loss = 0.3706, precision=84.4427%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 126,the train loss = 0.3512, the val loss = 0.3699, precision=84.4040%, recall=87.9435%, f1_score=87.9435%\n",
      "Epoch = 127,the train loss = 0.3504, the val loss = 0.3693, precision=84.4487%, recall=88.0242%, f1_score=88.0242%\n",
      "Epoch = 128,the train loss = 0.3500, the val loss = 0.3687, precision=84.4814%, recall=88.0242%, f1_score=88.0242%\n",
      "Epoch = 129,the train loss = 0.3492, the val loss = 0.3681, precision=84.5409%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 130,the train loss = 0.3482, the val loss = 0.3675, precision=84.5409%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 131,the train loss = 0.3478, the val loss = 0.3669, precision=84.5409%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 132,the train loss = 0.3472, the val loss = 0.3663, precision=84.5736%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 133,the train loss = 0.3467, the val loss = 0.3657, precision=84.5736%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 134,the train loss = 0.3454, the val loss = 0.3651, precision=84.6064%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 135,the train loss = 0.3447, the val loss = 0.3646, precision=84.6064%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 136,the train loss = 0.3450, the val loss = 0.3640, precision=84.6064%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 137,the train loss = 0.3441, the val loss = 0.3634, precision=84.6005%, recall=87.9435%, f1_score=87.9435%\n",
      "Epoch = 138,the train loss = 0.3432, the val loss = 0.3629, precision=84.6064%, recall=87.9839%, f1_score=87.9839%\n",
      "Epoch = 139,the train loss = 0.3419, the val loss = 0.3623, precision=84.6452%, recall=88.0242%, f1_score=88.0242%\n",
      "Epoch = 140,the train loss = 0.3417, the val loss = 0.3618, precision=84.6571%, recall=88.1048%, f1_score=88.1048%\n",
      "Epoch = 141,the train loss = 0.3412, the val loss = 0.3613, precision=84.6631%, recall=88.1452%, f1_score=88.1452%\n",
      "Epoch = 142,the train loss = 0.3403, the val loss = 0.3607, precision=84.6303%, recall=88.1452%, f1_score=88.1452%\n",
      "Epoch = 143,the train loss = 0.3396, the val loss = 0.3602, precision=84.6959%, recall=88.1452%, f1_score=88.1452%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 144,the train loss = 0.3392, the val loss = 0.3597, precision=84.7346%, recall=88.1855%, f1_score=88.1855%\n",
      "Epoch = 145,the train loss = 0.3388, the val loss = 0.3592, precision=84.7018%, recall=88.1855%, f1_score=88.1855%\n",
      "Epoch = 146,the train loss = 0.3381, the val loss = 0.3586, precision=84.7674%, recall=88.1855%, f1_score=88.1855%\n",
      "Epoch = 147,the train loss = 0.3377, the val loss = 0.3581, precision=84.8121%, recall=88.2661%, f1_score=88.2661%\n",
      "Epoch = 148,the train loss = 0.3366, the val loss = 0.3576, precision=84.8450%, recall=88.2661%, f1_score=88.2661%\n",
      "Epoch = 149,the train loss = 0.3368, the val loss = 0.3571, precision=84.8508%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 150,the train loss = 0.3355, the val loss = 0.3567, precision=84.8508%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 151,the train loss = 0.3352, the val loss = 0.3562, precision=84.8508%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 152,the train loss = 0.3349, the val loss = 0.3557, precision=84.8450%, recall=88.2661%, f1_score=88.2661%\n",
      "Epoch = 153,the train loss = 0.3340, the val loss = 0.3552, precision=84.8508%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 154,the train loss = 0.3340, the val loss = 0.3547, precision=84.9166%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 155,the train loss = 0.3334, the val loss = 0.3543, precision=84.9108%, recall=88.2661%, f1_score=88.2661%\n",
      "Epoch = 156,the train loss = 0.3325, the val loss = 0.3538, precision=84.9108%, recall=88.2661%, f1_score=88.2661%\n",
      "Epoch = 157,the train loss = 0.3324, the val loss = 0.3533, precision=84.9496%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 158,the train loss = 0.3314, the val loss = 0.3529, precision=84.9825%, recall=88.3065%, f1_score=88.3065%\n",
      "Epoch = 159,the train loss = 0.3310, the val loss = 0.3524, precision=84.9884%, recall=88.3468%, f1_score=88.3468%\n",
      "Epoch = 160,the train loss = 0.3303, the val loss = 0.3520, precision=85.0213%, recall=88.3468%, f1_score=88.3468%\n",
      "Epoch = 161,the train loss = 0.3303, the val loss = 0.3516, precision=85.0000%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 162,the train loss = 0.3293, the val loss = 0.3511, precision=85.0000%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 163,the train loss = 0.3293, the val loss = 0.3507, precision=85.0330%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 164,the train loss = 0.3286, the val loss = 0.3502, precision=85.0659%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 165,the train loss = 0.3285, the val loss = 0.3498, precision=85.0990%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 166,the train loss = 0.3276, the val loss = 0.3494, precision=85.0990%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 167,the train loss = 0.3273, the val loss = 0.3490, precision=85.1650%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 168,the train loss = 0.3262, the val loss = 0.3486, precision=85.1708%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 169,the train loss = 0.3263, the val loss = 0.3482, precision=85.2039%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 170,the train loss = 0.3254, the val loss = 0.3477, precision=85.2039%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 171,the train loss = 0.3250, the val loss = 0.3473, precision=85.2039%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 172,the train loss = 0.3246, the val loss = 0.3469, precision=85.1708%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 173,the train loss = 0.3239, the val loss = 0.3465, precision=85.1708%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 174,the train loss = 0.3243, the val loss = 0.3461, precision=85.1766%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 175,the train loss = 0.3228, the val loss = 0.3458, precision=85.1766%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 176,the train loss = 0.3225, the val loss = 0.3454, precision=85.1823%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 177,the train loss = 0.3222, the val loss = 0.3450, precision=85.2154%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 178,the train loss = 0.3219, the val loss = 0.3446, precision=85.2484%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 179,the train loss = 0.3211, the val loss = 0.3442, precision=85.2816%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 180,the train loss = 0.3209, the val loss = 0.3439, precision=85.2873%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 181,the train loss = 0.3200, the val loss = 0.3435, precision=85.2873%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 182,the train loss = 0.3200, the val loss = 0.3431, precision=85.2873%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 183,the train loss = 0.3192, the val loss = 0.3428, precision=85.2211%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 184,the train loss = 0.3192, the val loss = 0.3424, precision=85.2268%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 185,the train loss = 0.3188, the val loss = 0.3420, precision=85.2326%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 186,the train loss = 0.3185, the val loss = 0.3417, precision=85.2326%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 187,the train loss = 0.3187, the val loss = 0.3413, precision=85.2656%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 188,the train loss = 0.3174, the val loss = 0.3410, precision=85.2656%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 189,the train loss = 0.3167, the val loss = 0.3406, precision=85.2656%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 190,the train loss = 0.3166, the val loss = 0.3403, precision=85.2987%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 191,the train loss = 0.3156, the val loss = 0.3399, precision=85.3044%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 192,the train loss = 0.3154, the val loss = 0.3396, precision=85.3101%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 193,the train loss = 0.3148, the val loss = 0.3393, precision=85.3101%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 194,the train loss = 0.3154, the val loss = 0.3389, precision=85.3432%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 195,the train loss = 0.3143, the val loss = 0.3386, precision=85.3763%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 196,the train loss = 0.3140, the val loss = 0.3383, precision=85.4094%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 197,the train loss = 0.3137, the val loss = 0.3379, precision=85.4425%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 198,the train loss = 0.3138, the val loss = 0.3376, precision=85.4757%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 199,the train loss = 0.3127, the val loss = 0.3373, precision=85.5089%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 200,the train loss = 0.3126, the val loss = 0.3370, precision=85.5089%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 201,the train loss = 0.3124, the val loss = 0.3367, precision=85.4701%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 202,the train loss = 0.3120, the val loss = 0.3364, precision=85.4701%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 203,the train loss = 0.3112, the val loss = 0.3360, precision=85.4644%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 204,the train loss = 0.3107, the val loss = 0.3357, precision=85.4644%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 205,the train loss = 0.3105, the val loss = 0.3354, precision=85.5309%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 206,the train loss = 0.3098, the val loss = 0.3351, precision=85.5309%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 207,the train loss = 0.3099, the val loss = 0.3348, precision=85.5253%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 208,the train loss = 0.3095, the val loss = 0.3345, precision=85.5919%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 209,the train loss = 0.3089, the val loss = 0.3342, precision=85.6252%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 210,the train loss = 0.3088, the val loss = 0.3339, precision=85.6308%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 211,the train loss = 0.3079, the val loss = 0.3336, precision=85.6252%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 212,the train loss = 0.3076, the val loss = 0.3333, precision=85.6252%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 213,the train loss = 0.3079, the val loss = 0.3331, precision=85.6252%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 214,the train loss = 0.3068, the val loss = 0.3328, precision=85.6864%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 215,the train loss = 0.3068, the val loss = 0.3325, precision=85.6864%, recall=88.5887%, f1_score=88.5887%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 216,the train loss = 0.3064, the val loss = 0.3322, precision=85.7254%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 217,the train loss = 0.3059, the val loss = 0.3319, precision=85.7478%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 218,the train loss = 0.3055, the val loss = 0.3316, precision=85.7533%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 219,the train loss = 0.3058, the val loss = 0.3314, precision=85.7422%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 220,the train loss = 0.3050, the val loss = 0.3311, precision=85.7757%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 221,the train loss = 0.3043, the val loss = 0.3308, precision=85.7478%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 222,the train loss = 0.3040, the val loss = 0.3305, precision=85.7812%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 223,the train loss = 0.3042, the val loss = 0.3303, precision=85.8203%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 224,the train loss = 0.3034, the val loss = 0.3300, precision=85.8203%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 225,the train loss = 0.3034, the val loss = 0.3297, precision=85.8148%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 226,the train loss = 0.3029, the val loss = 0.3295, precision=85.8148%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 227,the train loss = 0.3027, the val loss = 0.3292, precision=85.8203%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 228,the train loss = 0.3024, the val loss = 0.3290, precision=85.8203%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 229,the train loss = 0.3018, the val loss = 0.3287, precision=85.8538%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 230,the train loss = 0.3022, the val loss = 0.3284, precision=85.8594%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 231,the train loss = 0.3014, the val loss = 0.3282, precision=85.8594%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 232,the train loss = 0.3010, the val loss = 0.3279, precision=85.8594%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 233,the train loss = 0.3009, the val loss = 0.3277, precision=85.8538%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 234,the train loss = 0.3007, the val loss = 0.3274, precision=85.8538%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 235,the train loss = 0.2999, the val loss = 0.3272, precision=85.8538%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 236,the train loss = 0.2996, the val loss = 0.3269, precision=85.8483%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 237,the train loss = 0.2990, the val loss = 0.3267, precision=85.8483%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 238,the train loss = 0.2991, the val loss = 0.3264, precision=85.8819%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 239,the train loss = 0.2985, the val loss = 0.3262, precision=85.8819%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 240,the train loss = 0.2987, the val loss = 0.3260, precision=85.8819%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 241,the train loss = 0.2980, the val loss = 0.3257, precision=85.8819%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 242,the train loss = 0.2981, the val loss = 0.3255, precision=85.8819%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 243,the train loss = 0.2979, the val loss = 0.3252, precision=85.8874%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 244,the train loss = 0.2970, the val loss = 0.3250, precision=85.8929%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 245,the train loss = 0.2973, the val loss = 0.3248, precision=85.8929%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 246,the train loss = 0.2964, the val loss = 0.3246, precision=85.8929%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 247,the train loss = 0.2969, the val loss = 0.3243, precision=85.8929%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 248,the train loss = 0.2964, the val loss = 0.3241, precision=85.8929%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 249,the train loss = 0.2956, the val loss = 0.3239, precision=85.8874%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 250,the train loss = 0.2953, the val loss = 0.3237, precision=85.8874%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 251,the train loss = 0.2951, the val loss = 0.3234, precision=85.9265%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 252,the train loss = 0.2949, the val loss = 0.3232, precision=85.9601%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 253,the train loss = 0.2952, the val loss = 0.3230, precision=85.9601%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 254,the train loss = 0.2945, the val loss = 0.3228, precision=85.9601%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 255,the train loss = 0.2944, the val loss = 0.3225, precision=85.9937%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 256,the train loss = 0.2936, the val loss = 0.3223, precision=86.0274%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 257,the train loss = 0.2934, the val loss = 0.3221, precision=86.0219%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 258,the train loss = 0.2932, the val loss = 0.3219, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 259,the train loss = 0.2932, the val loss = 0.3217, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 260,the train loss = 0.2924, the val loss = 0.3215, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 261,the train loss = 0.2930, the val loss = 0.3213, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 262,the train loss = 0.2930, the val loss = 0.3211, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 263,the train loss = 0.2920, the val loss = 0.3209, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 264,the train loss = 0.2913, the val loss = 0.3206, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 265,the train loss = 0.2916, the val loss = 0.3204, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 266,the train loss = 0.2918, the val loss = 0.3202, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 267,the train loss = 0.2910, the val loss = 0.3200, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 268,the train loss = 0.2908, the val loss = 0.3198, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 269,the train loss = 0.2906, the val loss = 0.3196, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 270,the train loss = 0.2900, the val loss = 0.3194, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 271,the train loss = 0.2894, the val loss = 0.3192, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 272,the train loss = 0.2897, the val loss = 0.3191, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 273,the train loss = 0.2893, the val loss = 0.3189, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 274,the train loss = 0.2897, the val loss = 0.3187, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 275,the train loss = 0.2883, the val loss = 0.3185, precision=86.0502%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 276,the train loss = 0.2887, the val loss = 0.3183, precision=86.0556%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 277,the train loss = 0.2889, the val loss = 0.3181, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 278,the train loss = 0.2876, the val loss = 0.3179, precision=86.0893%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 279,the train loss = 0.2879, the val loss = 0.3177, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 280,the train loss = 0.2878, the val loss = 0.3175, precision=86.0839%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 281,the train loss = 0.2873, the val loss = 0.3173, precision=86.1176%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 282,the train loss = 0.2868, the val loss = 0.3171, precision=86.1514%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 283,the train loss = 0.2863, the val loss = 0.3170, precision=86.1852%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 284,the train loss = 0.2875, the val loss = 0.3168, precision=86.2191%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 285,the train loss = 0.2864, the val loss = 0.3166, precision=86.2137%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 286,the train loss = 0.2860, the val loss = 0.3164, precision=86.2367%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 287,the train loss = 0.2857, the val loss = 0.3163, precision=86.2367%, recall=88.4274%, f1_score=88.4274%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 288,the train loss = 0.2852, the val loss = 0.3161, precision=86.2367%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 289,the train loss = 0.2850, the val loss = 0.3159, precision=86.2367%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 290,the train loss = 0.2852, the val loss = 0.3157, precision=86.2367%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 291,the train loss = 0.2849, the val loss = 0.3155, precision=86.2367%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 292,the train loss = 0.2845, the val loss = 0.3154, precision=86.2421%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 293,the train loss = 0.2842, the val loss = 0.3152, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 294,the train loss = 0.2842, the val loss = 0.3150, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 295,the train loss = 0.2836, the val loss = 0.3148, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 296,the train loss = 0.2835, the val loss = 0.3147, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 297,the train loss = 0.2829, the val loss = 0.3145, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 298,the train loss = 0.2827, the val loss = 0.3143, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 299,the train loss = 0.2830, the val loss = 0.3142, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 300,the train loss = 0.2829, the val loss = 0.3140, precision=86.2761%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 301,the train loss = 0.2825, the val loss = 0.3138, precision=86.3046%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 302,the train loss = 0.2824, the val loss = 0.3137, precision=86.3046%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 303,the train loss = 0.2820, the val loss = 0.3135, precision=86.3726%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 304,the train loss = 0.2818, the val loss = 0.3133, precision=86.3726%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 305,the train loss = 0.2810, the val loss = 0.3132, precision=86.3726%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 306,the train loss = 0.2814, the val loss = 0.3130, precision=86.3726%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 307,the train loss = 0.2812, the val loss = 0.3128, precision=86.4066%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 308,the train loss = 0.2807, the val loss = 0.3127, precision=86.4120%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 309,the train loss = 0.2812, the val loss = 0.3125, precision=86.4227%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 310,the train loss = 0.2802, the val loss = 0.3124, precision=86.4227%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 311,the train loss = 0.2803, the val loss = 0.3122, precision=86.4514%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 312,the train loss = 0.2798, the val loss = 0.3120, precision=86.4460%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 313,the train loss = 0.2801, the val loss = 0.3119, precision=86.4460%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 314,the train loss = 0.2795, the val loss = 0.3117, precision=86.4460%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 315,the train loss = 0.2794, the val loss = 0.3116, precision=86.5142%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 316,the train loss = 0.2793, the val loss = 0.3114, precision=86.5483%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 317,the train loss = 0.2789, the val loss = 0.3113, precision=86.5483%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 318,the train loss = 0.2793, the val loss = 0.3111, precision=86.5825%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 319,the train loss = 0.2786, the val loss = 0.3110, precision=86.5825%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 320,the train loss = 0.2778, the val loss = 0.3108, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 321,the train loss = 0.2778, the val loss = 0.3107, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 322,the train loss = 0.2782, the val loss = 0.3105, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 323,the train loss = 0.2782, the val loss = 0.3104, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 324,the train loss = 0.2776, the val loss = 0.3102, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 325,the train loss = 0.2775, the val loss = 0.3101, precision=86.5825%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 326,the train loss = 0.2769, the val loss = 0.3099, precision=86.5825%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 327,the train loss = 0.2765, the val loss = 0.3098, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 328,the train loss = 0.2768, the val loss = 0.3097, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 329,the train loss = 0.2762, the val loss = 0.3095, precision=86.6167%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 330,the train loss = 0.2759, the val loss = 0.3094, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 331,the train loss = 0.2763, the val loss = 0.3092, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 332,the train loss = 0.2757, the val loss = 0.3091, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 333,the train loss = 0.2754, the val loss = 0.3089, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 334,the train loss = 0.2752, the val loss = 0.3088, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 335,the train loss = 0.2753, the val loss = 0.3087, precision=86.6219%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 336,the train loss = 0.2747, the val loss = 0.3085, precision=86.6509%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 337,the train loss = 0.2746, the val loss = 0.3084, precision=86.6851%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 338,the train loss = 0.2753, the val loss = 0.3083, precision=86.6509%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 339,the train loss = 0.2741, the val loss = 0.3081, precision=86.6509%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 340,the train loss = 0.2739, the val loss = 0.3080, precision=86.6456%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 341,the train loss = 0.2741, the val loss = 0.3078, precision=86.6456%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 342,the train loss = 0.2734, the val loss = 0.3077, precision=86.6456%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 343,the train loss = 0.2740, the val loss = 0.3076, precision=86.6456%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 344,the train loss = 0.2738, the val loss = 0.3074, precision=86.6798%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 345,the train loss = 0.2737, the val loss = 0.3073, precision=86.6798%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 346,the train loss = 0.2728, the val loss = 0.3072, precision=86.6798%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 347,the train loss = 0.2726, the val loss = 0.3070, precision=86.6798%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 348,the train loss = 0.2728, the val loss = 0.3069, precision=86.6798%, recall=88.4274%, f1_score=88.4274%\n",
      "Epoch = 349,the train loss = 0.2722, the val loss = 0.3068, precision=86.6851%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 350,the train loss = 0.2725, the val loss = 0.3067, precision=86.6851%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 351,the train loss = 0.2721, the val loss = 0.3065, precision=86.7194%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 352,the train loss = 0.2715, the val loss = 0.3064, precision=86.7194%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 353,the train loss = 0.2721, the val loss = 0.3063, precision=86.7246%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 354,the train loss = 0.2721, the val loss = 0.3061, precision=86.7299%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 355,the train loss = 0.2716, the val loss = 0.3060, precision=86.7641%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 356,the train loss = 0.2712, the val loss = 0.3059, precision=86.7984%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 357,the train loss = 0.2706, the val loss = 0.3058, precision=86.7984%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 358,the train loss = 0.2707, the val loss = 0.3056, precision=86.7984%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 359,the train loss = 0.2706, the val loss = 0.3055, precision=86.7932%, recall=88.5081%, f1_score=88.5081%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 360,the train loss = 0.2702, the val loss = 0.3054, precision=86.7932%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 361,the train loss = 0.2706, the val loss = 0.3053, precision=86.7932%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 362,the train loss = 0.2702, the val loss = 0.3052, precision=86.7932%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 363,the train loss = 0.2703, the val loss = 0.3050, precision=86.7932%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 364,the train loss = 0.2697, the val loss = 0.3049, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 365,the train loss = 0.2691, the val loss = 0.3048, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 366,the train loss = 0.2699, the val loss = 0.3047, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 367,the train loss = 0.2694, the val loss = 0.3046, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 368,the train loss = 0.2691, the val loss = 0.3045, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 369,the train loss = 0.2688, the val loss = 0.3043, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 370,the train loss = 0.2686, the val loss = 0.3042, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 371,the train loss = 0.2685, the val loss = 0.3041, precision=86.8567%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 372,the train loss = 0.2686, the val loss = 0.3040, precision=86.8567%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 373,the train loss = 0.2684, the val loss = 0.3039, precision=86.8567%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 374,the train loss = 0.2680, the val loss = 0.3037, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 375,the train loss = 0.2675, the val loss = 0.3036, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 376,the train loss = 0.2679, the val loss = 0.3035, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 377,the train loss = 0.2679, the val loss = 0.3034, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 378,the train loss = 0.2673, the val loss = 0.3033, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 379,the train loss = 0.2676, the val loss = 0.3032, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 380,the train loss = 0.2668, the val loss = 0.3031, precision=86.8223%, recall=88.4677%, f1_score=88.4677%\n",
      "Epoch = 381,the train loss = 0.2671, the val loss = 0.3030, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 382,the train loss = 0.2670, the val loss = 0.3029, precision=86.8275%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 383,the train loss = 0.2665, the val loss = 0.3027, precision=86.8327%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 384,the train loss = 0.2663, the val loss = 0.3026, precision=86.8671%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 385,the train loss = 0.2663, the val loss = 0.3025, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 386,the train loss = 0.2660, the val loss = 0.3024, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 387,the train loss = 0.2656, the val loss = 0.3023, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 388,the train loss = 0.2660, the val loss = 0.3022, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 389,the train loss = 0.2659, the val loss = 0.3021, precision=86.8723%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 390,the train loss = 0.2660, the val loss = 0.3020, precision=86.8671%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 391,the train loss = 0.2655, the val loss = 0.3019, precision=86.8671%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 392,the train loss = 0.2652, the val loss = 0.3018, precision=86.8671%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 393,the train loss = 0.2657, the val loss = 0.3017, precision=86.8723%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 394,the train loss = 0.2646, the val loss = 0.3016, precision=86.9118%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 395,the train loss = 0.2650, the val loss = 0.3015, precision=86.9015%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 396,the train loss = 0.2642, the val loss = 0.3014, precision=86.9015%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 397,the train loss = 0.2645, the val loss = 0.3012, precision=86.9118%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 398,the train loss = 0.2640, the val loss = 0.3011, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 399,the train loss = 0.2639, the val loss = 0.3010, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 400,the train loss = 0.2642, the val loss = 0.3009, precision=86.9359%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 401,the train loss = 0.2638, the val loss = 0.3008, precision=86.9307%, recall=88.5081%, f1_score=88.5081%\n",
      "Epoch = 402,the train loss = 0.2632, the val loss = 0.3007, precision=86.9359%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 403,the train loss = 0.2639, the val loss = 0.3006, precision=86.9410%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 404,the train loss = 0.2630, the val loss = 0.3005, precision=86.9410%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 405,the train loss = 0.2632, the val loss = 0.3004, precision=86.9410%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 406,the train loss = 0.2629, the val loss = 0.3003, precision=86.9410%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 407,the train loss = 0.2626, the val loss = 0.3002, precision=86.9410%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 408,the train loss = 0.2634, the val loss = 0.3001, precision=86.9359%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 409,the train loss = 0.2627, the val loss = 0.3000, precision=86.9410%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 410,the train loss = 0.2620, the val loss = 0.2999, precision=86.9359%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 411,the train loss = 0.2622, the val loss = 0.2998, precision=86.9359%, recall=88.5484%, f1_score=88.5484%\n",
      "Epoch = 412,the train loss = 0.2619, the val loss = 0.2998, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 413,the train loss = 0.2617, the val loss = 0.2997, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 414,the train loss = 0.2618, the val loss = 0.2996, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 415,the train loss = 0.2613, the val loss = 0.2995, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 416,the train loss = 0.2622, the val loss = 0.2994, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 417,the train loss = 0.2613, the val loss = 0.2993, precision=86.9118%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 418,the train loss = 0.2611, the val loss = 0.2992, precision=86.9066%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 419,the train loss = 0.2612, the val loss = 0.2991, precision=86.8723%, recall=88.5887%, f1_score=88.5887%\n",
      "Epoch = 420,the train loss = 0.2609, the val loss = 0.2990, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 421,the train loss = 0.2610, the val loss = 0.2989, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 422,the train loss = 0.2606, the val loss = 0.2988, precision=86.8775%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 423,the train loss = 0.2601, the val loss = 0.2987, precision=86.9118%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 424,the train loss = 0.2601, the val loss = 0.2986, precision=86.9118%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 425,the train loss = 0.2603, the val loss = 0.2985, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 426,the train loss = 0.2597, the val loss = 0.2984, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 427,the train loss = 0.2595, the val loss = 0.2983, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 428,the train loss = 0.2597, the val loss = 0.2983, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 429,the train loss = 0.2595, the val loss = 0.2982, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 430,the train loss = 0.2589, the val loss = 0.2981, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 431,the train loss = 0.2589, the val loss = 0.2980, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 432,the train loss = 0.2596, the val loss = 0.2979, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 433,the train loss = 0.2586, the val loss = 0.2978, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 434,the train loss = 0.2590, the val loss = 0.2977, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 435,the train loss = 0.2585, the val loss = 0.2976, precision=86.9170%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 436,the train loss = 0.2583, the val loss = 0.2976, precision=86.9222%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 437,the train loss = 0.2581, the val loss = 0.2975, precision=86.9222%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 438,the train loss = 0.2583, the val loss = 0.2974, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 439,the train loss = 0.2579, the val loss = 0.2973, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 440,the train loss = 0.2573, the val loss = 0.2972, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 441,the train loss = 0.2577, the val loss = 0.2971, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 442,the train loss = 0.2580, the val loss = 0.2970, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 443,the train loss = 0.2573, the val loss = 0.2970, precision=86.9565%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 444,the train loss = 0.2574, the val loss = 0.2969, precision=86.9565%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 445,the train loss = 0.2570, the val loss = 0.2968, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 446,the train loss = 0.2568, the val loss = 0.2967, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 447,the train loss = 0.2569, the val loss = 0.2966, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 448,the train loss = 0.2564, the val loss = 0.2965, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 449,the train loss = 0.2563, the val loss = 0.2965, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 450,the train loss = 0.2563, the val loss = 0.2964, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 451,the train loss = 0.2567, the val loss = 0.2963, precision=86.9617%, recall=88.7500%, f1_score=88.7500%\n",
      "Epoch = 452,the train loss = 0.2560, the val loss = 0.2962, precision=86.9565%, recall=88.7097%, f1_score=88.7097%\n",
      "Epoch = 453,the train loss = 0.2562, the val loss = 0.2961, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 454,the train loss = 0.2574, the val loss = 0.2961, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 455,the train loss = 0.2557, the val loss = 0.2960, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 456,the train loss = 0.2555, the val loss = 0.2959, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 457,the train loss = 0.2558, the val loss = 0.2958, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 458,the train loss = 0.2551, the val loss = 0.2957, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 459,the train loss = 0.2556, the val loss = 0.2957, precision=86.9462%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 460,the train loss = 0.2555, the val loss = 0.2956, precision=86.9462%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 461,the train loss = 0.2550, the val loss = 0.2955, precision=86.9462%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 462,the train loss = 0.2548, the val loss = 0.2954, precision=86.9462%, recall=88.6290%, f1_score=88.6290%\n",
      "Epoch = 463,the train loss = 0.2552, the val loss = 0.2953, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 464,the train loss = 0.2549, the val loss = 0.2953, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 465,the train loss = 0.2549, the val loss = 0.2952, precision=86.9514%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 466,the train loss = 0.2541, the val loss = 0.2951, precision=86.9858%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 467,the train loss = 0.2544, the val loss = 0.2950, precision=86.9858%, recall=88.6694%, f1_score=88.6694%\n",
      "Epoch = 468,the train loss = 0.2544, the val loss = 0.2950, precision=86.9720%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 469,the train loss = 0.2539, the val loss = 0.2949, precision=86.9771%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 470,the train loss = 0.2542, the val loss = 0.2948, precision=86.9771%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 471,the train loss = 0.2540, the val loss = 0.2947, precision=86.9771%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 472,the train loss = 0.2532, the val loss = 0.2947, precision=86.9771%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 473,the train loss = 0.2534, the val loss = 0.2946, precision=86.9771%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 474,the train loss = 0.2536, the val loss = 0.2945, precision=87.0114%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 475,the train loss = 0.2531, the val loss = 0.2944, precision=87.0560%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 476,the train loss = 0.2541, the val loss = 0.2943, precision=87.0560%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 477,the train loss = 0.2530, the val loss = 0.2943, precision=87.0560%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 478,the train loss = 0.2529, the val loss = 0.2942, precision=87.0560%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 479,the train loss = 0.2525, the val loss = 0.2941, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 480,the train loss = 0.2524, the val loss = 0.2941, precision=87.1146%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 481,the train loss = 0.2523, the val loss = 0.2940, precision=87.1146%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 482,the train loss = 0.2525, the val loss = 0.2939, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 483,the train loss = 0.2523, the val loss = 0.2938, precision=87.1146%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 484,the train loss = 0.2525, the val loss = 0.2938, precision=87.1146%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 485,the train loss = 0.2519, the val loss = 0.2937, precision=87.1146%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 486,the train loss = 0.2521, the val loss = 0.2936, precision=87.1095%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 487,the train loss = 0.2516, the val loss = 0.2936, precision=87.1440%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 488,the train loss = 0.2516, the val loss = 0.2935, precision=87.1440%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 489,the train loss = 0.2517, the val loss = 0.2934, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 490,the train loss = 0.2512, the val loss = 0.2933, precision=87.1440%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 491,the train loss = 0.2515, the val loss = 0.2933, precision=87.1785%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 492,the train loss = 0.2515, the val loss = 0.2932, precision=87.1440%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 493,the train loss = 0.2508, the val loss = 0.2931, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 494,the train loss = 0.2507, the val loss = 0.2931, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 495,the train loss = 0.2510, the val loss = 0.2930, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 496,the train loss = 0.2500, the val loss = 0.2929, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 497,the train loss = 0.2510, the val loss = 0.2929, precision=87.1440%, recall=88.8306%, f1_score=88.8306%\n",
      "Epoch = 498,the train loss = 0.2502, the val loss = 0.2928, precision=87.1491%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 499,the train loss = 0.2503, the val loss = 0.2927, precision=87.1491%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 500,the train loss = 0.2498, the val loss = 0.2927, precision=87.1491%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 501,the train loss = 0.2498, the val loss = 0.2926, precision=87.1491%, recall=88.8710%, f1_score=88.8710%\n",
      "Epoch = 502,the train loss = 0.2498, the val loss = 0.2925, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 503,the train loss = 0.2499, the val loss = 0.2925, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 504,the train loss = 0.2498, the val loss = 0.2924, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 505,the train loss = 0.2495, the val loss = 0.2923, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 506,the train loss = 0.2499, the val loss = 0.2923, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 507,the train loss = 0.2494, the val loss = 0.2922, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 508,the train loss = 0.2488, the val loss = 0.2921, precision=87.1197%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 509,the train loss = 0.2492, the val loss = 0.2921, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 510,the train loss = 0.2487, the val loss = 0.2920, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 511,the train loss = 0.2492, the val loss = 0.2919, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 512,the train loss = 0.2486, the val loss = 0.2919, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 513,the train loss = 0.2486, the val loss = 0.2918, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 514,the train loss = 0.2483, the val loss = 0.2917, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 515,the train loss = 0.2487, the val loss = 0.2917, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 516,the train loss = 0.2482, the val loss = 0.2916, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 517,the train loss = 0.2483, the val loss = 0.2915, precision=87.1542%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 518,the train loss = 0.2482, the val loss = 0.2915, precision=87.1592%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 519,the train loss = 0.2481, the val loss = 0.2914, precision=87.1937%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 520,the train loss = 0.2476, the val loss = 0.2914, precision=87.1937%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 521,the train loss = 0.2477, the val loss = 0.2913, precision=87.1937%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 522,the train loss = 0.2476, the val loss = 0.2912, precision=87.1937%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 523,the train loss = 0.2476, the val loss = 0.2912, precision=87.1937%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 524,the train loss = 0.2477, the val loss = 0.2911, precision=87.1987%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 525,the train loss = 0.2472, the val loss = 0.2911, precision=87.1987%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 526,the train loss = 0.2474, the val loss = 0.2910, precision=87.2332%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 527,the train loss = 0.2478, the val loss = 0.2909, precision=87.2332%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 528,the train loss = 0.2474, the val loss = 0.2909, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 529,the train loss = 0.2474, the val loss = 0.2908, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 530,the train loss = 0.2465, the val loss = 0.2907, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 531,the train loss = 0.2466, the val loss = 0.2907, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 532,the train loss = 0.2463, the val loss = 0.2906, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 533,the train loss = 0.2465, the val loss = 0.2906, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 534,the train loss = 0.2459, the val loss = 0.2905, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 535,the train loss = 0.2462, the val loss = 0.2905, precision=87.2382%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 536,the train loss = 0.2459, the val loss = 0.2904, precision=87.2332%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 537,the train loss = 0.2460, the val loss = 0.2903, precision=87.2332%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 538,the train loss = 0.2460, the val loss = 0.2903, precision=87.2332%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 539,the train loss = 0.2458, the val loss = 0.2902, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 540,the train loss = 0.2456, the val loss = 0.2902, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 541,the train loss = 0.2453, the val loss = 0.2901, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 542,the train loss = 0.2458, the val loss = 0.2900, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 543,the train loss = 0.2455, the val loss = 0.2900, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 544,the train loss = 0.2453, the val loss = 0.2899, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 545,the train loss = 0.2455, the val loss = 0.2899, precision=87.2576%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 546,the train loss = 0.2446, the val loss = 0.2898, precision=87.2576%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 547,the train loss = 0.2447, the val loss = 0.2898, precision=87.2576%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 548,the train loss = 0.2447, the val loss = 0.2897, precision=87.2576%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 549,the train loss = 0.2448, the val loss = 0.2896, precision=87.2576%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 550,the train loss = 0.2443, the val loss = 0.2896, precision=87.2576%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 551,the train loss = 0.2443, the val loss = 0.2895, precision=87.2231%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 552,the train loss = 0.2453, the val loss = 0.2895, precision=87.2231%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 553,the train loss = 0.2442, the val loss = 0.2894, precision=87.2231%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 554,the train loss = 0.2442, the val loss = 0.2894, precision=87.2231%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 555,the train loss = 0.2443, the val loss = 0.2893, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 556,the train loss = 0.2434, the val loss = 0.2893, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 557,the train loss = 0.2439, the val loss = 0.2892, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 558,the train loss = 0.2434, the val loss = 0.2891, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 559,the train loss = 0.2437, the val loss = 0.2891, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 560,the train loss = 0.2435, the val loss = 0.2890, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 561,the train loss = 0.2432, the val loss = 0.2890, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 562,the train loss = 0.2432, the val loss = 0.2889, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 563,the train loss = 0.2439, the val loss = 0.2889, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 564,the train loss = 0.2430, the val loss = 0.2888, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 565,the train loss = 0.2429, the val loss = 0.2888, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 566,the train loss = 0.2431, the val loss = 0.2887, precision=87.2282%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 567,the train loss = 0.2428, the val loss = 0.2887, precision=87.2627%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 568,the train loss = 0.2423, the val loss = 0.2886, precision=87.2677%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 569,the train loss = 0.2432, the val loss = 0.2886, precision=87.2677%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 570,the train loss = 0.2425, the val loss = 0.2885, precision=87.2677%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 571,the train loss = 0.2423, the val loss = 0.2885, precision=87.2727%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 572,the train loss = 0.2421, the val loss = 0.2884, precision=87.2727%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 573,the train loss = 0.2422, the val loss = 0.2884, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 574,the train loss = 0.2426, the val loss = 0.2883, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 575,the train loss = 0.2420, the val loss = 0.2883, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 576,the train loss = 0.2422, the val loss = 0.2882, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 577,the train loss = 0.2423, the val loss = 0.2882, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 578,the train loss = 0.2423, the val loss = 0.2881, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 579,the train loss = 0.2419, the val loss = 0.2880, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 580,the train loss = 0.2415, the val loss = 0.2880, precision=87.3072%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 581,the train loss = 0.2413, the val loss = 0.2879, precision=87.3123%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 582,the train loss = 0.2418, the val loss = 0.2879, precision=87.3123%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 583,the train loss = 0.2413, the val loss = 0.2878, precision=87.3123%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 584,the train loss = 0.2410, the val loss = 0.2878, precision=87.3123%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 585,the train loss = 0.2410, the val loss = 0.2878, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 586,the train loss = 0.2415, the val loss = 0.2877, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 587,the train loss = 0.2407, the val loss = 0.2877, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 588,the train loss = 0.2407, the val loss = 0.2876, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 589,the train loss = 0.2407, the val loss = 0.2876, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 590,the train loss = 0.2404, the val loss = 0.2875, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 591,the train loss = 0.2402, the val loss = 0.2875, precision=87.3468%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 592,the train loss = 0.2405, the val loss = 0.2874, precision=87.3813%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 593,the train loss = 0.2402, the val loss = 0.2874, precision=87.3813%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 594,the train loss = 0.2403, the val loss = 0.2873, precision=87.3813%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 595,the train loss = 0.2404, the val loss = 0.2873, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 596,the train loss = 0.2399, the val loss = 0.2872, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 597,the train loss = 0.2398, the val loss = 0.2872, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 598,the train loss = 0.2398, the val loss = 0.2871, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 599,the train loss = 0.2394, the val loss = 0.2871, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 600,the train loss = 0.2395, the val loss = 0.2870, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 601,the train loss = 0.2393, the val loss = 0.2870, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 602,the train loss = 0.2390, the val loss = 0.2869, precision=87.3418%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 603,the train loss = 0.2394, the val loss = 0.2869, precision=87.3418%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 604,the train loss = 0.2392, the val loss = 0.2868, precision=87.3418%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 605,the train loss = 0.2395, the val loss = 0.2868, precision=87.3418%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 606,the train loss = 0.2387, the val loss = 0.2868, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 607,the train loss = 0.2389, the val loss = 0.2867, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 608,the train loss = 0.2399, the val loss = 0.2867, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 609,the train loss = 0.2390, the val loss = 0.2866, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 610,the train loss = 0.2384, the val loss = 0.2866, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 611,the train loss = 0.2391, the val loss = 0.2865, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 612,the train loss = 0.2385, the val loss = 0.2865, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 613,the train loss = 0.2382, the val loss = 0.2864, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 614,the train loss = 0.2382, the val loss = 0.2864, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 615,the train loss = 0.2385, the val loss = 0.2864, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 616,the train loss = 0.2378, the val loss = 0.2863, precision=87.3713%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 617,the train loss = 0.2381, the val loss = 0.2863, precision=87.3713%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 618,the train loss = 0.2380, the val loss = 0.2862, precision=87.3713%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 619,the train loss = 0.2384, the val loss = 0.2862, precision=87.3663%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 620,the train loss = 0.2380, the val loss = 0.2861, precision=87.3763%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 621,the train loss = 0.2377, the val loss = 0.2861, precision=87.3713%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 622,the train loss = 0.2378, the val loss = 0.2861, precision=87.4059%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 623,the train loss = 0.2374, the val loss = 0.2860, precision=87.4059%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 624,the train loss = 0.2377, the val loss = 0.2860, precision=87.4010%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 625,the train loss = 0.2378, the val loss = 0.2859, precision=87.3960%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 626,the train loss = 0.2370, the val loss = 0.2859, precision=87.3960%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 627,the train loss = 0.2372, the val loss = 0.2858, precision=87.3960%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 628,the train loss = 0.2373, the val loss = 0.2858, precision=87.3960%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 629,the train loss = 0.2368, the val loss = 0.2858, precision=87.3960%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 630,the train loss = 0.2367, the val loss = 0.2857, precision=87.4010%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 631,the train loss = 0.2365, the val loss = 0.2857, precision=87.4059%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 632,the train loss = 0.2373, the val loss = 0.2856, precision=87.4010%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 633,the train loss = 0.2365, the val loss = 0.2856, precision=87.4010%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 634,the train loss = 0.2365, the val loss = 0.2855, precision=87.4010%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 635,the train loss = 0.2363, the val loss = 0.2855, precision=87.4059%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 636,the train loss = 0.2361, the val loss = 0.2855, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 637,the train loss = 0.2361, the val loss = 0.2854, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 638,the train loss = 0.2363, the val loss = 0.2854, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 639,the train loss = 0.2359, the val loss = 0.2853, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 640,the train loss = 0.2361, the val loss = 0.2853, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 641,the train loss = 0.2361, the val loss = 0.2853, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 642,the train loss = 0.2358, the val loss = 0.2852, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 643,the train loss = 0.2356, the val loss = 0.2852, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 644,the train loss = 0.2356, the val loss = 0.2851, precision=87.4406%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 645,the train loss = 0.2360, the val loss = 0.2851, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 646,the train loss = 0.2355, the val loss = 0.2851, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 647,the train loss = 0.2354, the val loss = 0.2850, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 648,the train loss = 0.2354, the val loss = 0.2850, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 649,the train loss = 0.2351, the val loss = 0.2849, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 650,the train loss = 0.2347, the val loss = 0.2849, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 651,the train loss = 0.2351, the val loss = 0.2849, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 652,the train loss = 0.2348, the val loss = 0.2848, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 653,the train loss = 0.2360, the val loss = 0.2848, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 654,the train loss = 0.2350, the val loss = 0.2847, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 655,the train loss = 0.2344, the val loss = 0.2847, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 656,the train loss = 0.2345, the val loss = 0.2847, precision=87.4455%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 657,the train loss = 0.2347, the val loss = 0.2846, precision=87.4802%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 658,the train loss = 0.2344, the val loss = 0.2846, precision=87.4802%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 659,the train loss = 0.2342, the val loss = 0.2845, precision=87.4802%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 660,the train loss = 0.2342, the val loss = 0.2845, precision=87.4802%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 661,the train loss = 0.2346, the val loss = 0.2845, precision=87.4802%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 662,the train loss = 0.2340, the val loss = 0.2844, precision=87.4802%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 663,the train loss = 0.2339, the val loss = 0.2844, precision=87.4752%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 664,the train loss = 0.2345, the val loss = 0.2844, precision=87.4752%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 665,the train loss = 0.2352, the val loss = 0.2843, precision=87.4752%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 666,the train loss = 0.2337, the val loss = 0.2843, precision=87.4752%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 667,the train loss = 0.2339, the val loss = 0.2842, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 668,the train loss = 0.2335, the val loss = 0.2842, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 669,the train loss = 0.2332, the val loss = 0.2842, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 670,the train loss = 0.2329, the val loss = 0.2841, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 671,the train loss = 0.2337, the val loss = 0.2841, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 672,the train loss = 0.2332, the val loss = 0.2841, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 673,the train loss = 0.2333, the val loss = 0.2840, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 674,the train loss = 0.2332, the val loss = 0.2840, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 675,the train loss = 0.2329, the val loss = 0.2840, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 676,the train loss = 0.2336, the val loss = 0.2839, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 677,the train loss = 0.2332, the val loss = 0.2839, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 678,the train loss = 0.2323, the val loss = 0.2838, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 679,the train loss = 0.2324, the val loss = 0.2838, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 680,the train loss = 0.2325, the val loss = 0.2838, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 681,the train loss = 0.2323, the val loss = 0.2837, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 682,the train loss = 0.2323, the val loss = 0.2837, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 683,the train loss = 0.2335, the val loss = 0.2837, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 684,the train loss = 0.2321, the val loss = 0.2836, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 685,the train loss = 0.2326, the val loss = 0.2836, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 686,the train loss = 0.2330, the val loss = 0.2836, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 687,the train loss = 0.2326, the val loss = 0.2835, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 688,the train loss = 0.2318, the val loss = 0.2835, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 689,the train loss = 0.2323, the val loss = 0.2835, precision=87.5099%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 690,the train loss = 0.2326, the val loss = 0.2834, precision=87.5446%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 691,the train loss = 0.2321, the val loss = 0.2834, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 692,the train loss = 0.2317, the val loss = 0.2833, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 693,the train loss = 0.2314, the val loss = 0.2833, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 694,the train loss = 0.2317, the val loss = 0.2833, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 695,the train loss = 0.2314, the val loss = 0.2832, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 696,the train loss = 0.2312, the val loss = 0.2832, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 697,the train loss = 0.2313, the val loss = 0.2832, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 698,the train loss = 0.2313, the val loss = 0.2831, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 699,the train loss = 0.2310, the val loss = 0.2831, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 700,the train loss = 0.2315, the val loss = 0.2831, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 701,the train loss = 0.2315, the val loss = 0.2830, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 702,the train loss = 0.2308, the val loss = 0.2830, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 703,the train loss = 0.2314, the val loss = 0.2830, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 704,the train loss = 0.2312, the val loss = 0.2829, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 705,the train loss = 0.2308, the val loss = 0.2829, precision=87.5149%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 706,the train loss = 0.2311, the val loss = 0.2829, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 707,the train loss = 0.2307, the val loss = 0.2829, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 708,the train loss = 0.2304, the val loss = 0.2828, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 709,the train loss = 0.2306, the val loss = 0.2828, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 710,the train loss = 0.2304, the val loss = 0.2828, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 711,the train loss = 0.2301, the val loss = 0.2827, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 712,the train loss = 0.2302, the val loss = 0.2827, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 713,the train loss = 0.2304, the val loss = 0.2827, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 714,the train loss = 0.2302, the val loss = 0.2826, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 715,the train loss = 0.2295, the val loss = 0.2826, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 716,the train loss = 0.2301, the val loss = 0.2826, precision=87.5496%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 717,the train loss = 0.2300, the val loss = 0.2825, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 718,the train loss = 0.2297, the val loss = 0.2825, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 719,the train loss = 0.2292, the val loss = 0.2825, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 720,the train loss = 0.2294, the val loss = 0.2824, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 721,the train loss = 0.2293, the val loss = 0.2824, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 722,the train loss = 0.2296, the val loss = 0.2824, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 723,the train loss = 0.2294, the val loss = 0.2824, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 724,the train loss = 0.2301, the val loss = 0.2823, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 725,the train loss = 0.2299, the val loss = 0.2823, precision=87.5843%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 726,the train loss = 0.2290, the val loss = 0.2823, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 727,the train loss = 0.2293, the val loss = 0.2822, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 728,the train loss = 0.2288, the val loss = 0.2822, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 729,the train loss = 0.2286, the val loss = 0.2822, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 730,the train loss = 0.2288, the val loss = 0.2821, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 731,the train loss = 0.2293, the val loss = 0.2821, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 732,the train loss = 0.2284, the val loss = 0.2821, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 733,the train loss = 0.2284, the val loss = 0.2821, precision=87.5892%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 734,the train loss = 0.2286, the val loss = 0.2820, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 735,the train loss = 0.2281, the val loss = 0.2820, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 736,the train loss = 0.2285, the val loss = 0.2820, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 737,the train loss = 0.2280, the val loss = 0.2819, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 738,the train loss = 0.2283, the val loss = 0.2819, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 739,the train loss = 0.2277, the val loss = 0.2819, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 740,the train loss = 0.2281, the val loss = 0.2818, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 741,the train loss = 0.2277, the val loss = 0.2818, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 742,the train loss = 0.2275, the val loss = 0.2818, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 743,the train loss = 0.2282, the val loss = 0.2818, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 744,the train loss = 0.2279, the val loss = 0.2817, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 745,the train loss = 0.2279, the val loss = 0.2817, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 746,the train loss = 0.2277, the val loss = 0.2817, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 747,the train loss = 0.2276, the val loss = 0.2816, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 748,the train loss = 0.2275, the val loss = 0.2816, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 749,the train loss = 0.2276, the val loss = 0.2816, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 750,the train loss = 0.2279, the val loss = 0.2816, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 751,the train loss = 0.2275, the val loss = 0.2815, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 752,the train loss = 0.2275, the val loss = 0.2815, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 753,the train loss = 0.2272, the val loss = 0.2815, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 754,the train loss = 0.2267, the val loss = 0.2814, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 755,the train loss = 0.2276, the val loss = 0.2814, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 756,the train loss = 0.2269, the val loss = 0.2814, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 757,the train loss = 0.2271, the val loss = 0.2814, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 758,the train loss = 0.2268, the val loss = 0.2813, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 759,the train loss = 0.2265, the val loss = 0.2813, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 760,the train loss = 0.2269, the val loss = 0.2813, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 761,the train loss = 0.2264, the val loss = 0.2813, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 762,the train loss = 0.2268, the val loss = 0.2812, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 763,the train loss = 0.2267, the val loss = 0.2812, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 764,the train loss = 0.2260, the val loss = 0.2812, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 765,the train loss = 0.2269, the val loss = 0.2811, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 766,the train loss = 0.2267, the val loss = 0.2811, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 767,the train loss = 0.2258, the val loss = 0.2811, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 768,the train loss = 0.2256, the val loss = 0.2811, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 769,the train loss = 0.2266, the val loss = 0.2810, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 770,the train loss = 0.2259, the val loss = 0.2810, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 771,the train loss = 0.2257, the val loss = 0.2810, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 772,the train loss = 0.2264, the val loss = 0.2810, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 773,the train loss = 0.2258, the val loss = 0.2809, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 774,the train loss = 0.2256, the val loss = 0.2809, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 775,the train loss = 0.2256, the val loss = 0.2809, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 776,the train loss = 0.2253, the val loss = 0.2809, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 777,the train loss = 0.2260, the val loss = 0.2808, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 778,the train loss = 0.2259, the val loss = 0.2808, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 779,the train loss = 0.2253, the val loss = 0.2808, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 780,the train loss = 0.2258, the val loss = 0.2807, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 781,the train loss = 0.2253, the val loss = 0.2807, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 782,the train loss = 0.2253, the val loss = 0.2807, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 783,the train loss = 0.2249, the val loss = 0.2807, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 784,the train loss = 0.2252, the val loss = 0.2807, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 785,the train loss = 0.2252, the val loss = 0.2806, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 786,the train loss = 0.2254, the val loss = 0.2806, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 787,the train loss = 0.2253, the val loss = 0.2806, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 788,the train loss = 0.2251, the val loss = 0.2806, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 789,the train loss = 0.2246, the val loss = 0.2805, precision=87.6587%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 790,the train loss = 0.2242, the val loss = 0.2805, precision=87.6240%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 791,the train loss = 0.2249, the val loss = 0.2805, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 792,the train loss = 0.2242, the val loss = 0.2805, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 793,the train loss = 0.2244, the val loss = 0.2804, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 794,the train loss = 0.2240, the val loss = 0.2804, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 795,the train loss = 0.2242, the val loss = 0.2804, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 796,the train loss = 0.2249, the val loss = 0.2804, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 797,the train loss = 0.2257, the val loss = 0.2803, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 798,the train loss = 0.2241, the val loss = 0.2803, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 799,the train loss = 0.2238, the val loss = 0.2803, precision=87.6636%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 800,the train loss = 0.2238, the val loss = 0.2803, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 801,the train loss = 0.2238, the val loss = 0.2802, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 802,the train loss = 0.2234, the val loss = 0.2802, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 803,the train loss = 0.2246, the val loss = 0.2802, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 804,the train loss = 0.2237, the val loss = 0.2802, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 805,the train loss = 0.2247, the val loss = 0.2801, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 806,the train loss = 0.2235, the val loss = 0.2801, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 807,the train loss = 0.2242, the val loss = 0.2801, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 808,the train loss = 0.2233, the val loss = 0.2801, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 809,the train loss = 0.2232, the val loss = 0.2801, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 810,the train loss = 0.2229, the val loss = 0.2800, precision=87.6685%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 811,the train loss = 0.2231, the val loss = 0.2800, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 812,the train loss = 0.2228, the val loss = 0.2800, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 813,the train loss = 0.2234, the val loss = 0.2800, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 814,the train loss = 0.2225, the val loss = 0.2799, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 815,the train loss = 0.2226, the val loss = 0.2799, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 816,the train loss = 0.2230, the val loss = 0.2799, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 817,the train loss = 0.2228, the val loss = 0.2799, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 818,the train loss = 0.2227, the val loss = 0.2798, precision=87.6289%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 819,the train loss = 0.2228, the val loss = 0.2798, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 820,the train loss = 0.2225, the val loss = 0.2798, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 821,the train loss = 0.2226, the val loss = 0.2798, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 822,the train loss = 0.2226, the val loss = 0.2798, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 823,the train loss = 0.2228, the val loss = 0.2797, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 824,the train loss = 0.2223, the val loss = 0.2797, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 825,the train loss = 0.2224, the val loss = 0.2797, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 826,the train loss = 0.2222, the val loss = 0.2797, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 827,the train loss = 0.2220, the val loss = 0.2796, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 828,the train loss = 0.2223, the val loss = 0.2796, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 829,the train loss = 0.2219, the val loss = 0.2796, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 830,the train loss = 0.2221, the val loss = 0.2796, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 831,the train loss = 0.2225, the val loss = 0.2796, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 832,the train loss = 0.2220, the val loss = 0.2795, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 833,the train loss = 0.2223, the val loss = 0.2795, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 834,the train loss = 0.2222, the val loss = 0.2795, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 835,the train loss = 0.2222, the val loss = 0.2795, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 836,the train loss = 0.2221, the val loss = 0.2795, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 837,the train loss = 0.2221, the val loss = 0.2794, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 838,the train loss = 0.2216, the val loss = 0.2794, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 839,the train loss = 0.2214, the val loss = 0.2794, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 840,the train loss = 0.2209, the val loss = 0.2794, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 841,the train loss = 0.2209, the val loss = 0.2793, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 842,the train loss = 0.2214, the val loss = 0.2793, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 843,the train loss = 0.2216, the val loss = 0.2793, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 844,the train loss = 0.2212, the val loss = 0.2793, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 845,the train loss = 0.2220, the val loss = 0.2793, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 846,the train loss = 0.2214, the val loss = 0.2792, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 847,the train loss = 0.2211, the val loss = 0.2792, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 848,the train loss = 0.2214, the val loss = 0.2792, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 849,the train loss = 0.2209, the val loss = 0.2792, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 850,the train loss = 0.2211, the val loss = 0.2792, precision=87.6734%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 851,the train loss = 0.2210, the val loss = 0.2791, precision=87.6734%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 852,the train loss = 0.2204, the val loss = 0.2791, precision=87.6734%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 853,the train loss = 0.2206, the val loss = 0.2791, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 854,the train loss = 0.2202, the val loss = 0.2791, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 855,the train loss = 0.2201, the val loss = 0.2791, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 856,the train loss = 0.2203, the val loss = 0.2790, precision=87.6338%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 857,the train loss = 0.2213, the val loss = 0.2790, precision=87.6387%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 858,the train loss = 0.2203, the val loss = 0.2790, precision=87.6040%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 859,the train loss = 0.2201, the val loss = 0.2790, precision=87.6040%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 860,the train loss = 0.2196, the val loss = 0.2790, precision=87.6040%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 861,the train loss = 0.2211, the val loss = 0.2789, precision=87.6040%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 862,the train loss = 0.2207, the val loss = 0.2789, precision=87.6040%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 863,the train loss = 0.2198, the val loss = 0.2789, precision=87.6040%, recall=89.1935%, f1_score=89.1935%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 864,the train loss = 0.2200, the val loss = 0.2789, precision=87.6089%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 865,the train loss = 0.2203, the val loss = 0.2789, precision=87.6089%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 866,the train loss = 0.2198, the val loss = 0.2788, precision=87.6089%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 867,the train loss = 0.2196, the val loss = 0.2788, precision=87.6089%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 868,the train loss = 0.2197, the val loss = 0.2788, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 869,the train loss = 0.2196, the val loss = 0.2788, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 870,the train loss = 0.2196, the val loss = 0.2788, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 871,the train loss = 0.2200, the val loss = 0.2787, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 872,the train loss = 0.2192, the val loss = 0.2787, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 873,the train loss = 0.2201, the val loss = 0.2787, precision=87.5889%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 874,the train loss = 0.2192, the val loss = 0.2787, precision=87.5889%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 875,the train loss = 0.2194, the val loss = 0.2787, precision=87.5938%, recall=89.3952%, f1_score=89.3952%\n",
      "Epoch = 876,the train loss = 0.2194, the val loss = 0.2787, precision=87.5938%, recall=89.3952%, f1_score=89.3952%\n",
      "Epoch = 877,the train loss = 0.2187, the val loss = 0.2786, precision=87.5938%, recall=89.3952%, f1_score=89.3952%\n",
      "Epoch = 878,the train loss = 0.2194, the val loss = 0.2786, precision=87.5889%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 879,the train loss = 0.2190, the val loss = 0.2786, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 880,the train loss = 0.2189, the val loss = 0.2786, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 881,the train loss = 0.2189, the val loss = 0.2786, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 882,the train loss = 0.2190, the val loss = 0.2785, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 883,the train loss = 0.2188, the val loss = 0.2785, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 884,the train loss = 0.2188, the val loss = 0.2785, precision=87.6236%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 885,the train loss = 0.2186, the val loss = 0.2785, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 886,the train loss = 0.2186, the val loss = 0.2785, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 887,the train loss = 0.2190, the val loss = 0.2785, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 888,the train loss = 0.2186, the val loss = 0.2784, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 889,the train loss = 0.2192, the val loss = 0.2784, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 890,the train loss = 0.2185, the val loss = 0.2784, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 891,the train loss = 0.2182, the val loss = 0.2784, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 892,the train loss = 0.2179, the val loss = 0.2784, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 893,the train loss = 0.2197, the val loss = 0.2784, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 894,the train loss = 0.2185, the val loss = 0.2783, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 895,the train loss = 0.2182, the val loss = 0.2783, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 896,the train loss = 0.2182, the val loss = 0.2783, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 897,the train loss = 0.2181, the val loss = 0.2783, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 898,the train loss = 0.2182, the val loss = 0.2783, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 899,the train loss = 0.2178, the val loss = 0.2783, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 900,the train loss = 0.2181, the val loss = 0.2782, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 901,the train loss = 0.2176, the val loss = 0.2782, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 902,the train loss = 0.2176, the val loss = 0.2782, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 903,the train loss = 0.2178, the val loss = 0.2782, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 904,the train loss = 0.2174, the val loss = 0.2782, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 905,the train loss = 0.2182, the val loss = 0.2782, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 906,the train loss = 0.2175, the val loss = 0.2781, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 907,the train loss = 0.2178, the val loss = 0.2781, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 908,the train loss = 0.2173, the val loss = 0.2781, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 909,the train loss = 0.2183, the val loss = 0.2781, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 910,the train loss = 0.2177, the val loss = 0.2781, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 911,the train loss = 0.2176, the val loss = 0.2780, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 912,the train loss = 0.2175, the val loss = 0.2780, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 913,the train loss = 0.2166, the val loss = 0.2780, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 914,the train loss = 0.2176, the val loss = 0.2780, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 915,the train loss = 0.2170, the val loss = 0.2780, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 916,the train loss = 0.2170, the val loss = 0.2780, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 917,the train loss = 0.2169, the val loss = 0.2779, precision=87.6187%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 918,the train loss = 0.2170, the val loss = 0.2779, precision=87.6187%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 919,the train loss = 0.2175, the val loss = 0.2779, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 920,the train loss = 0.2167, the val loss = 0.2779, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 921,the train loss = 0.2166, the val loss = 0.2779, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 922,the train loss = 0.2165, the val loss = 0.2779, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 923,the train loss = 0.2172, the val loss = 0.2779, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 924,the train loss = 0.2174, the val loss = 0.2778, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 925,the train loss = 0.2163, the val loss = 0.2778, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 926,the train loss = 0.2165, the val loss = 0.2778, precision=87.6880%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 927,the train loss = 0.2167, the val loss = 0.2778, precision=87.6880%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 928,the train loss = 0.2162, the val loss = 0.2778, precision=87.6880%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 929,the train loss = 0.2162, the val loss = 0.2778, precision=87.6929%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 930,the train loss = 0.2161, the val loss = 0.2777, precision=87.6929%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 931,the train loss = 0.2164, the val loss = 0.2777, precision=87.6929%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 932,the train loss = 0.2165, the val loss = 0.2777, precision=87.6929%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 933,the train loss = 0.2159, the val loss = 0.2777, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 934,the train loss = 0.2159, the val loss = 0.2777, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 935,the train loss = 0.2157, the val loss = 0.2777, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 936,the train loss = 0.2159, the val loss = 0.2777, precision=87.6582%, recall=89.3548%, f1_score=89.3548%\n",
      "Epoch = 937,the train loss = 0.2157, the val loss = 0.2776, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 938,the train loss = 0.2157, the val loss = 0.2776, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 939,the train loss = 0.2159, the val loss = 0.2776, precision=87.6533%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 940,the train loss = 0.2162, the val loss = 0.2776, precision=87.6832%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 941,the train loss = 0.2162, the val loss = 0.2776, precision=87.6187%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 942,the train loss = 0.2160, the val loss = 0.2776, precision=87.6187%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 943,the train loss = 0.2150, the val loss = 0.2775, precision=87.5791%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 944,the train loss = 0.2160, the val loss = 0.2775, precision=87.6485%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 945,the train loss = 0.2157, the val loss = 0.2775, precision=87.6485%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 946,the train loss = 0.2153, the val loss = 0.2775, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 947,the train loss = 0.2154, the val loss = 0.2775, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 948,the train loss = 0.2154, the val loss = 0.2775, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 949,the train loss = 0.2151, the val loss = 0.2775, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 950,the train loss = 0.2154, the val loss = 0.2774, precision=87.6485%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 951,the train loss = 0.2152, the val loss = 0.2774, precision=87.5791%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 952,the train loss = 0.2147, the val loss = 0.2774, precision=87.6138%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 953,the train loss = 0.2147, the val loss = 0.2774, precision=87.6485%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 954,the train loss = 0.2154, the val loss = 0.2774, precision=87.6485%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 955,the train loss = 0.2147, the val loss = 0.2774, precision=87.6485%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 956,the train loss = 0.2152, the val loss = 0.2774, precision=87.6832%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 957,the train loss = 0.2147, the val loss = 0.2773, precision=87.6783%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 958,the train loss = 0.2145, the val loss = 0.2773, precision=87.6783%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 959,the train loss = 0.2148, the val loss = 0.2773, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 960,the train loss = 0.2149, the val loss = 0.2773, precision=87.6783%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 961,the train loss = 0.2145, the val loss = 0.2773, precision=87.6783%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 962,the train loss = 0.2145, the val loss = 0.2773, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 963,the train loss = 0.2150, the val loss = 0.2773, precision=87.6832%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 964,the train loss = 0.2142, the val loss = 0.2772, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 965,the train loss = 0.2146, the val loss = 0.2772, precision=87.7179%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 966,the train loss = 0.2141, the val loss = 0.2772, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 967,the train loss = 0.2146, the val loss = 0.2772, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 968,the train loss = 0.2139, the val loss = 0.2772, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 969,the train loss = 0.2137, the val loss = 0.2772, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 970,the train loss = 0.2144, the val loss = 0.2772, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 971,the train loss = 0.2141, the val loss = 0.2771, precision=87.7575%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 972,the train loss = 0.2142, the val loss = 0.2771, precision=87.7575%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 973,the train loss = 0.2134, the val loss = 0.2771, precision=87.7228%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 974,the train loss = 0.2137, the val loss = 0.2771, precision=87.7228%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 975,the train loss = 0.2142, the val loss = 0.2771, precision=87.7923%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 976,the train loss = 0.2135, the val loss = 0.2771, precision=87.7575%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 977,the train loss = 0.2140, the val loss = 0.2771, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 978,the train loss = 0.2136, the val loss = 0.2771, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 979,the train loss = 0.2139, the val loss = 0.2770, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 980,the train loss = 0.2135, the val loss = 0.2770, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 981,the train loss = 0.2143, the val loss = 0.2770, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 982,the train loss = 0.2132, the val loss = 0.2770, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 983,the train loss = 0.2135, the val loss = 0.2770, precision=87.8223%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 984,the train loss = 0.2136, the val loss = 0.2770, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 985,the train loss = 0.2129, the val loss = 0.2770, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 986,the train loss = 0.2131, the val loss = 0.2770, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 987,the train loss = 0.2133, the val loss = 0.2769, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 988,the train loss = 0.2133, the val loss = 0.2769, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 989,the train loss = 0.2133, the val loss = 0.2769, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 990,the train loss = 0.2127, the val loss = 0.2769, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 991,the train loss = 0.2131, the val loss = 0.2769, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 992,the train loss = 0.2128, the val loss = 0.2769, precision=87.8223%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 993,the train loss = 0.2133, the val loss = 0.2769, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 994,the train loss = 0.2127, the val loss = 0.2768, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 995,the train loss = 0.2128, the val loss = 0.2768, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 996,the train loss = 0.2127, the val loss = 0.2768, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 997,the train loss = 0.2124, the val loss = 0.2768, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 998,the train loss = 0.2123, the val loss = 0.2768, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 999,the train loss = 0.2128, the val loss = 0.2768, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1000,the train loss = 0.2132, the val loss = 0.2768, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1001,the train loss = 0.2123, the val loss = 0.2768, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1002,the train loss = 0.2130, the val loss = 0.2767, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1003,the train loss = 0.2126, the val loss = 0.2767, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1004,the train loss = 0.2123, the val loss = 0.2767, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1005,the train loss = 0.2125, the val loss = 0.2767, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1006,the train loss = 0.2126, the val loss = 0.2767, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1007,the train loss = 0.2125, the val loss = 0.2767, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1008,the train loss = 0.2122, the val loss = 0.2767, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1009,the train loss = 0.2122, the val loss = 0.2767, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1010,the train loss = 0.2121, the val loss = 0.2766, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1011,the train loss = 0.2126, the val loss = 0.2766, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1012,the train loss = 0.2119, the val loss = 0.2766, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1013,the train loss = 0.2118, the val loss = 0.2766, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1014,the train loss = 0.2118, the val loss = 0.2766, precision=87.8175%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1015,the train loss = 0.2118, the val loss = 0.2766, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1016,the train loss = 0.2115, the val loss = 0.2766, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1017,the train loss = 0.2122, the val loss = 0.2766, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1018,the train loss = 0.2115, the val loss = 0.2766, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1019,the train loss = 0.2126, the val loss = 0.2765, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1020,the train loss = 0.2118, the val loss = 0.2765, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1021,the train loss = 0.2119, the val loss = 0.2765, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1022,the train loss = 0.2121, the val loss = 0.2765, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1023,the train loss = 0.2114, the val loss = 0.2765, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1024,the train loss = 0.2113, the val loss = 0.2765, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1025,the train loss = 0.2115, the val loss = 0.2765, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1026,the train loss = 0.2110, the val loss = 0.2765, precision=87.7527%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1027,the train loss = 0.2116, the val loss = 0.2765, precision=87.7875%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1028,the train loss = 0.2107, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1029,the train loss = 0.2109, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1030,the train loss = 0.2111, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1031,the train loss = 0.2110, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1032,the train loss = 0.2109, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1033,the train loss = 0.2112, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1034,the train loss = 0.2104, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1035,the train loss = 0.2106, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1036,the train loss = 0.2110, the val loss = 0.2764, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1037,the train loss = 0.2105, the val loss = 0.2763, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1038,the train loss = 0.2115, the val loss = 0.2763, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1039,the train loss = 0.2117, the val loss = 0.2763, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1040,the train loss = 0.2114, the val loss = 0.2763, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1041,the train loss = 0.2105, the val loss = 0.2763, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1042,the train loss = 0.2104, the val loss = 0.2763, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1043,the train loss = 0.2105, the val loss = 0.2763, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1044,the train loss = 0.2103, the val loss = 0.2763, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1045,the train loss = 0.2102, the val loss = 0.2763, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1046,the train loss = 0.2107, the val loss = 0.2762, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1047,the train loss = 0.2106, the val loss = 0.2762, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1048,the train loss = 0.2101, the val loss = 0.2762, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1049,the train loss = 0.2102, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1050,the train loss = 0.2101, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1051,the train loss = 0.2101, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1052,the train loss = 0.2098, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1053,the train loss = 0.2104, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1054,the train loss = 0.2099, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1055,the train loss = 0.2102, the val loss = 0.2762, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1056,the train loss = 0.2094, the val loss = 0.2761, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1057,the train loss = 0.2101, the val loss = 0.2761, precision=87.7778%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1058,the train loss = 0.2097, the val loss = 0.2761, precision=87.7430%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1059,the train loss = 0.2093, the val loss = 0.2761, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1060,the train loss = 0.2095, the val loss = 0.2761, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1061,the train loss = 0.2099, the val loss = 0.2761, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1062,the train loss = 0.2103, the val loss = 0.2761, precision=87.6783%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1063,the train loss = 0.2098, the val loss = 0.2761, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1064,the train loss = 0.2097, the val loss = 0.2761, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1065,the train loss = 0.2096, the val loss = 0.2761, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1066,the train loss = 0.2091, the val loss = 0.2761, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1067,the train loss = 0.2094, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1068,the train loss = 0.2092, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1069,the train loss = 0.2092, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1070,the train loss = 0.2095, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1071,the train loss = 0.2091, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1072,the train loss = 0.2093, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1073,the train loss = 0.2102, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1074,the train loss = 0.2088, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1075,the train loss = 0.2092, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1076,the train loss = 0.2089, the val loss = 0.2760, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1077,the train loss = 0.2089, the val loss = 0.2759, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1078,the train loss = 0.2087, the val loss = 0.2759, precision=87.7130%, recall=89.2339%, f1_score=89.2339%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1079,the train loss = 0.2088, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1080,the train loss = 0.2084, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1081,the train loss = 0.2084, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1082,the train loss = 0.2083, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1083,the train loss = 0.2087, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1084,the train loss = 0.2091, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1085,the train loss = 0.2089, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1086,the train loss = 0.2089, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1087,the train loss = 0.2091, the val loss = 0.2759, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1088,the train loss = 0.2087, the val loss = 0.2758, precision=87.7478%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1089,the train loss = 0.2082, the val loss = 0.2758, precision=87.7826%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1090,the train loss = 0.2080, the val loss = 0.2758, precision=87.8523%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1091,the train loss = 0.2083, the val loss = 0.2758, precision=87.8523%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1092,the train loss = 0.2082, the val loss = 0.2758, precision=87.8523%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1093,the train loss = 0.2085, the val loss = 0.2758, precision=87.8523%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1094,the train loss = 0.2084, the val loss = 0.2758, precision=87.8872%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1095,the train loss = 0.2080, the val loss = 0.2758, precision=87.8872%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1096,the train loss = 0.2084, the val loss = 0.2758, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1097,the train loss = 0.2082, the val loss = 0.2758, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1098,the train loss = 0.2077, the val loss = 0.2758, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1099,the train loss = 0.2080, the val loss = 0.2758, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1100,the train loss = 0.2078, the val loss = 0.2757, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1101,the train loss = 0.2075, the val loss = 0.2757, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1102,the train loss = 0.2080, the val loss = 0.2757, precision=87.8920%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1103,the train loss = 0.2079, the val loss = 0.2757, precision=87.9269%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1104,the train loss = 0.2076, the val loss = 0.2757, precision=87.9619%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1105,the train loss = 0.2080, the val loss = 0.2757, precision=87.9619%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1106,the train loss = 0.2076, the val loss = 0.2757, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1107,the train loss = 0.2078, the val loss = 0.2757, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1108,the train loss = 0.2071, the val loss = 0.2757, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1109,the train loss = 0.2074, the val loss = 0.2757, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1110,the train loss = 0.2076, the val loss = 0.2757, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1111,the train loss = 0.2075, the val loss = 0.2757, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1112,the train loss = 0.2070, the val loss = 0.2756, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1113,the train loss = 0.2077, the val loss = 0.2756, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1114,the train loss = 0.2074, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1115,the train loss = 0.2073, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1116,the train loss = 0.2070, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1117,the train loss = 0.2070, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1118,the train loss = 0.2068, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1119,the train loss = 0.2070, the val loss = 0.2756, precision=87.9221%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1120,the train loss = 0.2072, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1121,the train loss = 0.2069, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1122,the train loss = 0.2067, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1123,the train loss = 0.2073, the val loss = 0.2756, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1124,the train loss = 0.2070, the val loss = 0.2756, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1125,the train loss = 0.2068, the val loss = 0.2755, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1126,the train loss = 0.2072, the val loss = 0.2755, precision=87.9920%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1127,the train loss = 0.2067, the val loss = 0.2755, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1128,the train loss = 0.2067, the val loss = 0.2755, precision=87.9920%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1129,the train loss = 0.2075, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1130,the train loss = 0.2063, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1131,the train loss = 0.2065, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1132,the train loss = 0.2066, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1133,the train loss = 0.2064, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1134,the train loss = 0.2068, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1135,the train loss = 0.2067, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1136,the train loss = 0.2064, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1137,the train loss = 0.2061, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1138,the train loss = 0.2060, the val loss = 0.2755, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1139,the train loss = 0.2070, the val loss = 0.2754, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1140,the train loss = 0.2063, the val loss = 0.2754, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1141,the train loss = 0.2060, the val loss = 0.2754, precision=87.9523%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1142,the train loss = 0.2069, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1143,the train loss = 0.2059, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1144,the train loss = 0.2056, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1145,the train loss = 0.2057, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1146,the train loss = 0.2055, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1147,the train loss = 0.2056, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1148,the train loss = 0.2057, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1149,the train loss = 0.2053, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1150,the train loss = 0.2063, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1151,the train loss = 0.2054, the val loss = 0.2754, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1152,the train loss = 0.2052, the val loss = 0.2753, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1153,the train loss = 0.2056, the val loss = 0.2753, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1154,the train loss = 0.2053, the val loss = 0.2753, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1155,the train loss = 0.2057, the val loss = 0.2753, precision=87.9571%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1156,the train loss = 0.2063, the val loss = 0.2753, precision=87.9920%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1157,the train loss = 0.2057, the val loss = 0.2753, precision=87.9920%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1158,the train loss = 0.2062, the val loss = 0.2753, precision=87.9873%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1159,the train loss = 0.2051, the val loss = 0.2753, precision=87.9873%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1160,the train loss = 0.2052, the val loss = 0.2753, precision=87.9873%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1161,the train loss = 0.2051, the val loss = 0.2753, precision=87.9873%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1162,the train loss = 0.2052, the val loss = 0.2753, precision=88.0270%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1163,the train loss = 0.2053, the val loss = 0.2753, precision=88.0270%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1164,the train loss = 0.2052, the val loss = 0.2753, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1165,the train loss = 0.2054, the val loss = 0.2753, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1166,the train loss = 0.2054, the val loss = 0.2753, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1167,the train loss = 0.2054, the val loss = 0.2753, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1168,the train loss = 0.2048, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1169,the train loss = 0.2061, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1170,the train loss = 0.2048, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1171,the train loss = 0.2048, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1172,the train loss = 0.2041, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1173,the train loss = 0.2045, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1174,the train loss = 0.2044, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1175,the train loss = 0.2049, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1176,the train loss = 0.2042, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1177,the train loss = 0.2044, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1178,the train loss = 0.2043, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1179,the train loss = 0.2044, the val loss = 0.2752, precision=88.0621%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1180,the train loss = 0.2043, the val loss = 0.2752, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1181,the train loss = 0.2045, the val loss = 0.2752, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1182,the train loss = 0.2043, the val loss = 0.2752, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1183,the train loss = 0.2050, the val loss = 0.2752, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1184,the train loss = 0.2040, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1185,the train loss = 0.2047, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1186,the train loss = 0.2044, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1187,the train loss = 0.2042, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1188,the train loss = 0.2041, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1189,the train loss = 0.2039, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1190,the train loss = 0.2043, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1191,the train loss = 0.2040, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1192,the train loss = 0.2034, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1193,the train loss = 0.2037, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1194,the train loss = 0.2040, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1195,the train loss = 0.2037, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1196,the train loss = 0.2042, the val loss = 0.2751, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1197,the train loss = 0.2036, the val loss = 0.2751, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1198,the train loss = 0.2034, the val loss = 0.2751, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1199,the train loss = 0.2038, the val loss = 0.2751, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1200,the train loss = 0.2040, the val loss = 0.2751, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1201,the train loss = 0.2033, the val loss = 0.2750, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1202,the train loss = 0.2039, the val loss = 0.2750, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1203,the train loss = 0.2033, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1204,the train loss = 0.2033, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1205,the train loss = 0.2032, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1206,the train loss = 0.2030, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1207,the train loss = 0.2039, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1208,the train loss = 0.2032, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1209,the train loss = 0.2036, the val loss = 0.2750, precision=88.0366%, recall=89.3145%, f1_score=89.3145%\n",
      "Epoch = 1210,the train loss = 0.2028, the val loss = 0.2750, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1211,the train loss = 0.2035, the val loss = 0.2750, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1212,the train loss = 0.2032, the val loss = 0.2750, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1213,the train loss = 0.2032, the val loss = 0.2750, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1214,the train loss = 0.2041, the val loss = 0.2750, precision=88.0318%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1215,the train loss = 0.2035, the val loss = 0.2750, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1216,the train loss = 0.2028, the val loss = 0.2750, precision=88.0668%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1217,the train loss = 0.2031, the val loss = 0.2750, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1218,the train loss = 0.2032, the val loss = 0.2750, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1219,the train loss = 0.2028, the val loss = 0.2750, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1220,the train loss = 0.2029, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1221,the train loss = 0.2028, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1222,the train loss = 0.2032, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1223,the train loss = 0.2029, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1224,the train loss = 0.2027, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1225,the train loss = 0.2024, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1226,the train loss = 0.2028, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1227,the train loss = 0.2027, the val loss = 0.2749, precision=88.1019%, recall=89.2742%, f1_score=89.2742%\n",
      "Epoch = 1228,the train loss = 0.2031, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1229,the train loss = 0.2026, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1230,the train loss = 0.2019, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1231,the train loss = 0.2029, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1232,the train loss = 0.2021, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1233,the train loss = 0.2031, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1234,the train loss = 0.2024, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1235,the train loss = 0.2021, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1236,the train loss = 0.2017, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1237,the train loss = 0.2023, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1238,the train loss = 0.2021, the val loss = 0.2749, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1239,the train loss = 0.2019, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1240,the train loss = 0.2021, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1241,the train loss = 0.2018, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1242,the train loss = 0.2018, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1243,the train loss = 0.2018, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1244,the train loss = 0.2015, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1245,the train loss = 0.2018, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1246,the train loss = 0.2024, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1247,the train loss = 0.2019, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1248,the train loss = 0.2025, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1249,the train loss = 0.2025, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1250,the train loss = 0.2016, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1251,the train loss = 0.2019, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1252,the train loss = 0.2013, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1253,the train loss = 0.2011, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1254,the train loss = 0.2016, the val loss = 0.2748, precision=88.0971%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1255,the train loss = 0.2017, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1256,the train loss = 0.2011, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1257,the train loss = 0.2016, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1258,the train loss = 0.2014, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1259,the train loss = 0.2014, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1260,the train loss = 0.2015, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1261,the train loss = 0.2014, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1262,the train loss = 0.2007, the val loss = 0.2748, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1263,the train loss = 0.2017, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1264,the train loss = 0.2013, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1265,the train loss = 0.2008, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1266,the train loss = 0.2011, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1267,the train loss = 0.2013, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1268,the train loss = 0.2007, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1269,the train loss = 0.2017, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1270,the train loss = 0.2011, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1271,the train loss = 0.2011, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1272,the train loss = 0.2011, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1273,the train loss = 0.2007, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1274,the train loss = 0.2007, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1275,the train loss = 0.2010, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1276,the train loss = 0.2007, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1277,the train loss = 0.2005, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1278,the train loss = 0.2009, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1279,the train loss = 0.2009, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1280,the train loss = 0.2010, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1281,the train loss = 0.2003, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1282,the train loss = 0.2002, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1283,the train loss = 0.2008, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1284,the train loss = 0.2005, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1285,the train loss = 0.2002, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1286,the train loss = 0.2000, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1287,the train loss = 0.2008, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1288,the train loss = 0.2002, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1289,the train loss = 0.2006, the val loss = 0.2747, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1290,the train loss = 0.1999, the val loss = 0.2746, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1291,the train loss = 0.2001, the val loss = 0.2746, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1292,the train loss = 0.1998, the val loss = 0.2746, precision=88.1322%, recall=89.2339%, f1_score=89.2339%\n",
      "Epoch = 1293,the train loss = 0.2005, the val loss = 0.2746, precision=88.1275%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1294,the train loss = 0.1998, the val loss = 0.2746, precision=88.1275%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1295,the train loss = 0.2005, the val loss = 0.2746, precision=88.1275%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1296,the train loss = 0.1994, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1297,the train loss = 0.1999, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1298,the train loss = 0.2001, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1299,the train loss = 0.1999, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1300,the train loss = 0.1996, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1301,the train loss = 0.2000, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1302,the train loss = 0.2002, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1303,the train loss = 0.1998, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1304,the train loss = 0.1995, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1305,the train loss = 0.1995, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1306,the train loss = 0.1998, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1307,the train loss = 0.1993, the val loss = 0.2746, precision=88.1626%, recall=89.1935%, f1_score=89.1935%\n",
      "Epoch = 1308,the train loss = 0.1998, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1309,the train loss = 0.2001, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1310,the train loss = 0.1993, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1311,the train loss = 0.1996, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1312,the train loss = 0.1999, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1313,the train loss = 0.1994, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1314,the train loss = 0.1995, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1315,the train loss = 0.1993, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1316,the train loss = 0.1991, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1317,the train loss = 0.1992, the val loss = 0.2746, precision=88.1579%, recall=89.1532%, f1_score=89.1532%\n",
      "Epoch = 1318,the train loss = 0.1994, the val loss = 0.2746, precision=88.1532%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1319,the train loss = 0.1992, the val loss = 0.2746, precision=88.1532%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1320,the train loss = 0.1989, the val loss = 0.2746, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1321,the train loss = 0.1994, the val loss = 0.2746, precision=88.1532%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1322,the train loss = 0.1989, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1323,the train loss = 0.1989, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1324,the train loss = 0.1992, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1325,the train loss = 0.1991, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1326,the train loss = 0.1989, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1327,the train loss = 0.1987, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1328,the train loss = 0.1992, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1329,the train loss = 0.1993, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1330,the train loss = 0.1988, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1331,the train loss = 0.1987, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1332,the train loss = 0.1986, the val loss = 0.2745, precision=88.1180%, recall=89.1129%, f1_score=89.1129%\n",
      "Epoch = 1333,the train loss = 0.1987, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1334,the train loss = 0.1987, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1335,the train loss = 0.1990, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1336,the train loss = 0.1991, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1337,the train loss = 0.1981, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1338,the train loss = 0.1984, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1339,the train loss = 0.1986, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1340,the train loss = 0.1988, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1341,the train loss = 0.1982, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1342,the train loss = 0.1990, the val loss = 0.2745, precision=88.1133%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1343,the train loss = 0.1989, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1344,the train loss = 0.1982, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1345,the train loss = 0.1979, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1346,the train loss = 0.1980, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1347,the train loss = 0.1978, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1348,the train loss = 0.1985, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1349,the train loss = 0.1981, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1350,the train loss = 0.1982, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1351,the train loss = 0.1977, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1352,the train loss = 0.1982, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1353,the train loss = 0.1986, the val loss = 0.2745, precision=88.1085%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1354,the train loss = 0.1976, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1355,the train loss = 0.1982, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1356,the train loss = 0.1980, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1357,the train loss = 0.1975, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1358,the train loss = 0.1977, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1359,the train loss = 0.1975, the val loss = 0.2745, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1360,the train loss = 0.1977, the val loss = 0.2745, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1361,the train loss = 0.1975, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1362,the train loss = 0.1980, the val loss = 0.2745, precision=88.1437%, recall=89.0323%, f1_score=89.0323%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1363,the train loss = 0.1973, the val loss = 0.2745, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1364,the train loss = 0.1977, the val loss = 0.2745, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1365,the train loss = 0.1978, the val loss = 0.2745, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1366,the train loss = 0.1978, the val loss = 0.2745, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1367,the train loss = 0.1981, the val loss = 0.2744, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1368,the train loss = 0.1975, the val loss = 0.2744, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1369,the train loss = 0.1976, the val loss = 0.2744, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1370,the train loss = 0.1974, the val loss = 0.2744, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1371,the train loss = 0.1969, the val loss = 0.2744, precision=88.1484%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1372,the train loss = 0.1974, the val loss = 0.2744, precision=88.1836%, recall=89.0726%, f1_score=89.0726%\n",
      "Epoch = 1373,the train loss = 0.1972, the val loss = 0.2744, precision=88.2141%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1374,the train loss = 0.1977, the val loss = 0.2744, precision=88.2141%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1375,the train loss = 0.1970, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1376,the train loss = 0.1969, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1377,the train loss = 0.1971, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1378,the train loss = 0.1969, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1379,the train loss = 0.1966, the val loss = 0.2744, precision=88.2141%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1380,the train loss = 0.1967, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1381,the train loss = 0.1967, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1382,the train loss = 0.1969, the val loss = 0.2744, precision=88.1742%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1383,the train loss = 0.1972, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1384,the train loss = 0.1968, the val loss = 0.2744, precision=88.1789%, recall=89.0323%, f1_score=89.0323%\n",
      "Epoch = 1385,the train loss = 0.1974, the val loss = 0.2744, precision=88.1742%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1386,the train loss = 0.1967, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1387,the train loss = 0.1970, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1388,the train loss = 0.1967, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1389,the train loss = 0.1969, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1390,the train loss = 0.1965, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1391,the train loss = 0.1971, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1392,the train loss = 0.1968, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1393,the train loss = 0.1973, the val loss = 0.2744, precision=88.1695%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1394,the train loss = 0.1963, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1395,the train loss = 0.1960, the val loss = 0.2744, precision=88.2400%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1396,the train loss = 0.1961, the val loss = 0.2744, precision=88.2400%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1397,the train loss = 0.1963, the val loss = 0.2744, precision=88.2400%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1398,the train loss = 0.1961, the val loss = 0.2744, precision=88.2400%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1399,the train loss = 0.1962, the val loss = 0.2744, precision=88.2400%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1400,the train loss = 0.1961, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1401,the train loss = 0.1961, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1402,the train loss = 0.1962, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1403,the train loss = 0.1959, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1404,the train loss = 0.1959, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1405,the train loss = 0.1963, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1406,the train loss = 0.1962, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1407,the train loss = 0.1968, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1408,the train loss = 0.1959, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1409,the train loss = 0.1957, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1410,the train loss = 0.1957, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1411,the train loss = 0.1963, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1412,the train loss = 0.1963, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1413,the train loss = 0.1958, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1414,the train loss = 0.1959, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1415,the train loss = 0.1960, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1416,the train loss = 0.1959, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1417,the train loss = 0.1962, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1418,the train loss = 0.1960, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1419,the train loss = 0.1958, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1420,the train loss = 0.1960, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1421,the train loss = 0.1956, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1422,the train loss = 0.1958, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1423,the train loss = 0.1958, the val loss = 0.2744, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1424,the train loss = 0.1965, the val loss = 0.2744, precision=88.2000%, recall=88.9113%, f1_score=88.9113%\n",
      "Epoch = 1425,the train loss = 0.1952, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1426,the train loss = 0.1953, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1427,the train loss = 0.1952, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1428,the train loss = 0.1950, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1429,the train loss = 0.1956, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1430,the train loss = 0.1953, the val loss = 0.2744, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1431,the train loss = 0.1957, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1432,the train loss = 0.1956, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1433,the train loss = 0.1956, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 1434,the train loss = 0.1953, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1435,the train loss = 0.1947, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1436,the train loss = 0.1953, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1437,the train loss = 0.1953, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1438,the train loss = 0.1950, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1439,the train loss = 0.1957, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1440,the train loss = 0.1949, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1441,the train loss = 0.1951, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1442,the train loss = 0.1948, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1443,the train loss = 0.1946, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1444,the train loss = 0.1946, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1445,the train loss = 0.1947, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1446,the train loss = 0.1950, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1447,the train loss = 0.1950, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1448,the train loss = 0.1946, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1449,the train loss = 0.1943, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1450,the train loss = 0.1948, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1451,the train loss = 0.1949, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1452,the train loss = 0.1946, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1453,the train loss = 0.1949, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1454,the train loss = 0.1947, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1455,the train loss = 0.1950, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1456,the train loss = 0.1944, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1457,the train loss = 0.1943, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1458,the train loss = 0.1944, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1459,the train loss = 0.1944, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1460,the train loss = 0.1945, the val loss = 0.2743, precision=88.2047%, recall=88.9516%, f1_score=88.9516%\n",
      "Epoch = 1461,the train loss = 0.1944, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1462,the train loss = 0.1940, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1463,the train loss = 0.1944, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1464,the train loss = 0.1942, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1465,the train loss = 0.1939, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1466,the train loss = 0.1942, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1467,the train loss = 0.1939, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Epoch = 1468,the train loss = 0.1943, the val loss = 0.2743, precision=88.2094%, recall=88.9919%, f1_score=88.9919%\n",
      "Validation performance didn't improve for 10 epochs. Training stops.\n"
     ]
    }
   ],
   "source": [
    "label = df['sentiment'].values\n",
    "\n",
    "train_x, trian_y, val_x, val_y = split_data(data_tfidf, label)\n",
    "\n",
    "w, train_all_loss, val_all_loss, precision_list, recall_list, f1_score_list = train(train_x,\n",
    "                                                 trian_y,\n",
    "                                                 val_x,\n",
    "                                                 val_y,\n",
    "                                                 early_stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T03:19:28.526211Z",
     "start_time": "2020-08-05T03:19:27.858560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Loss Curves')\n",
    "plt.plot(\n",
    "    np.arange(len(train_all_loss)),\n",
    "              train_all_loss,\n",
    "              color='green',\n",
    "              label='training loss')\n",
    "plt.plot(\n",
    "    np.arange(len(val_all_loss)), val_all_loss, color='red',\n",
    "              label='valid loss')\n",
    "plt.legend()  # 显示图例\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T03:19:28.963948Z",
     "start_time": "2020-08-05T03:19:28.532209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Loss Curves')\n",
    "plt.plot(np.arange(len(precision_list)),\n",
    "         precision_list,\n",
    "         color='green',\n",
    "         label='precision')\n",
    "plt.plot(np.arange(len(recall_list)), recall_list, color='red', label='recall')\n",
    "\n",
    "plt.plot(np.arange(len(f1_score_list)), f1_score_list, color='b', label='recall')\n",
    "plt.legend()  # 显示图例\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考链接\n",
    "https://www.cnblogs.com/Luv-GEM/p/10888026.html\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/98061179\n",
    "\n",
    "https://github.com/FudanNLP/nlp-beginner/blob/master/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "522px",
    "left": "1201px",
    "right": "20px",
    "top": "139px",
    "width": "704px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

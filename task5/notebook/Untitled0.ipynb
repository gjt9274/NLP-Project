{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1pczWHG0Htls",
    "outputId": "c98af3fd-1f2e-43fa-a5f1-5f8fda06e224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fajlm1cMEqxJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPsiZHYUEudQ"
   },
   "outputs": [],
   "source": [
    "# 读入预处理的数据\n",
    "datas = np.load(\"/content/tang.npz\")\n",
    "data = datas['data']\n",
    "ix2word = datas['ix2word'].item()\n",
    "word2ix = datas['word2ix'].item()\n",
    "    \n",
    "# 转为torch.Tensor\n",
    "data = torch.from_numpy(data)\n",
    "train_loader = DataLoader(data, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1Mmljy_EoQ_"
   },
   "outputs": [],
   "source": [
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=3)\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim, 512), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Linear(512, 2048), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(2048, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, input, hidden = None):\n",
    "        seq_len, batch_size = input.size()\n",
    "        \n",
    "        if hidden is None:\n",
    "            h_0 = input.data.new(3, batch_size, self.hidden_dim).fill_(0).float()\n",
    "            c_0 = input.data.new(3, batch_size, self.hidden_dim).fill_(0).float()\n",
    "        else:\n",
    "            h_0, c_0 = hidden\n",
    "\n",
    "        embeds = self.embedding(input)\n",
    "        output, hidden = self.lstm(embeds, (h_0, c_0))\n",
    "        output = self.classifier(output.view(seq_len * batch_size, -1))\n",
    "        \n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_B6OdcXaGL9W"
   },
   "outputs": [],
   "source": [
    "# 配置模型，是否继续上一次的训练\n",
    "model = PoetryModel(len(word2ix),embedding_dim = 128,hidden_dim = 256)\n",
    "\n",
    "model_path = ''         # 预训练模型路径\n",
    "if model_path:\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=5e-3, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vymheOExGOID"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, ix2word, word2ix, device, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, data in enumerate(dataloader):\n",
    "        data = data.long().transpose(1, 0).contiguous()\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        input, target = data[:-1, :], data[1:, :]\n",
    "        output, _ = model(input)\n",
    "        loss = criterion(output, target.view(-1))\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "            \n",
    "        if (batch_idx+1) % 200 == 0:\n",
    "            print('train epoch: {} [{}/{} ({:.0f}%)]\\tloss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data[1]), len(dataloader.dataset),\n",
    "                100. * batch_idx / len(dataloader), loss.item()))\n",
    "            \n",
    "    train_loss *= BATCH_SIZE\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('\\ntrain epoch: {}\\t average loss: {:.6f}\\n'.format(epoch,train_loss))\n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jYPJpkh_GRes",
    "outputId": "c98cf136-8743-46b1-d82d-ee81ef756db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch: 1 [3184/57363 (6%)]\tloss: 2.457983\n",
      "train epoch: 1 [6384/57363 (11%)]\tloss: 2.984989\n",
      "train epoch: 1 [9584/57363 (17%)]\tloss: 2.935027\n",
      "train epoch: 1 [12784/57363 (22%)]\tloss: 2.889703\n",
      "train epoch: 1 [15984/57363 (28%)]\tloss: 2.445836\n",
      "train epoch: 1 [19184/57363 (33%)]\tloss: 2.264923\n",
      "train epoch: 1 [22384/57363 (39%)]\tloss: 2.731436\n",
      "train epoch: 1 [25584/57363 (45%)]\tloss: 3.021433\n",
      "train epoch: 1 [28784/57363 (50%)]\tloss: 2.057563\n",
      "train epoch: 1 [31984/57363 (56%)]\tloss: 2.127482\n",
      "train epoch: 1 [35184/57363 (61%)]\tloss: 2.487499\n",
      "train epoch: 1 [38384/57363 (67%)]\tloss: 2.053689\n",
      "train epoch: 1 [41584/57363 (72%)]\tloss: 2.421721\n",
      "train epoch: 1 [44784/57363 (78%)]\tloss: 2.931567\n",
      "train epoch: 1 [47984/57363 (84%)]\tloss: 1.913436\n",
      "train epoch: 1 [51184/57363 (89%)]\tloss: 1.969792\n",
      "train epoch: 1 [54384/57363 (95%)]\tloss: 2.248346\n",
      "\n",
      "train epoch: 1\t average loss: 2.480166\n",
      "\n",
      "train epoch: 2 [3184/57363 (6%)]\tloss: 1.955216\n",
      "train epoch: 2 [6384/57363 (11%)]\tloss: 2.063302\n",
      "train epoch: 2 [9584/57363 (17%)]\tloss: 1.753361\n",
      "train epoch: 2 [12784/57363 (22%)]\tloss: 2.082960\n",
      "train epoch: 2 [15984/57363 (28%)]\tloss: 2.552140\n",
      "train epoch: 2 [19184/57363 (33%)]\tloss: 1.933926\n",
      "train epoch: 2 [22384/57363 (39%)]\tloss: 1.992898\n",
      "train epoch: 2 [25584/57363 (45%)]\tloss: 2.279721\n",
      "train epoch: 2 [28784/57363 (50%)]\tloss: 1.945700\n",
      "train epoch: 2 [31984/57363 (56%)]\tloss: 2.550801\n",
      "train epoch: 2 [35184/57363 (61%)]\tloss: 2.354236\n",
      "train epoch: 2 [38384/57363 (67%)]\tloss: 2.458277\n",
      "train epoch: 2 [41584/57363 (72%)]\tloss: 1.693927\n",
      "train epoch: 2 [44784/57363 (78%)]\tloss: 2.421438\n",
      "train epoch: 2 [47984/57363 (84%)]\tloss: 2.068186\n",
      "train epoch: 2 [51184/57363 (89%)]\tloss: 1.760582\n",
      "train epoch: 2 [54384/57363 (95%)]\tloss: 2.298029\n",
      "\n",
      "train epoch: 2\t average loss: 2.200592\n",
      "\n",
      "train epoch: 3 [3184/57363 (6%)]\tloss: 2.268663\n",
      "train epoch: 3 [6384/57363 (11%)]\tloss: 1.695242\n",
      "train epoch: 3 [9584/57363 (17%)]\tloss: 2.363932\n",
      "train epoch: 3 [12784/57363 (22%)]\tloss: 2.256745\n",
      "train epoch: 3 [15984/57363 (28%)]\tloss: 2.952091\n",
      "train epoch: 3 [19184/57363 (33%)]\tloss: 2.539191\n",
      "train epoch: 3 [22384/57363 (39%)]\tloss: 1.833947\n",
      "train epoch: 3 [25584/57363 (45%)]\tloss: 2.198897\n",
      "train epoch: 3 [28784/57363 (50%)]\tloss: 2.755451\n",
      "train epoch: 3 [31984/57363 (56%)]\tloss: 1.886878\n",
      "train epoch: 3 [35184/57363 (61%)]\tloss: 2.177857\n",
      "train epoch: 3 [38384/57363 (67%)]\tloss: 2.277328\n",
      "train epoch: 3 [41584/57363 (72%)]\tloss: 1.934600\n",
      "train epoch: 3 [44784/57363 (78%)]\tloss: 2.090788\n",
      "train epoch: 3 [47984/57363 (84%)]\tloss: 1.433599\n",
      "train epoch: 3 [51184/57363 (89%)]\tloss: 2.209145\n",
      "train epoch: 3 [54384/57363 (95%)]\tloss: 2.212497\n",
      "\n",
      "train epoch: 3\t average loss: 2.105640\n",
      "\n",
      "train epoch: 4 [3184/57363 (6%)]\tloss: 2.313629\n",
      "train epoch: 4 [6384/57363 (11%)]\tloss: 1.951228\n",
      "train epoch: 4 [9584/57363 (17%)]\tloss: 1.619512\n",
      "train epoch: 4 [12784/57363 (22%)]\tloss: 2.274040\n",
      "train epoch: 4 [15984/57363 (28%)]\tloss: 2.000770\n",
      "train epoch: 4 [19184/57363 (33%)]\tloss: 1.873215\n",
      "train epoch: 4 [22384/57363 (39%)]\tloss: 1.896302\n",
      "train epoch: 4 [25584/57363 (45%)]\tloss: 1.963047\n",
      "train epoch: 4 [28784/57363 (50%)]\tloss: 1.945997\n",
      "train epoch: 4 [31984/57363 (56%)]\tloss: 2.236700\n",
      "train epoch: 4 [35184/57363 (61%)]\tloss: 1.528618\n",
      "train epoch: 4 [38384/57363 (67%)]\tloss: 1.954107\n",
      "train epoch: 4 [41584/57363 (72%)]\tloss: 1.649721\n",
      "train epoch: 4 [44784/57363 (78%)]\tloss: 1.910145\n",
      "train epoch: 4 [47984/57363 (84%)]\tloss: 2.160519\n",
      "train epoch: 4 [51184/57363 (89%)]\tloss: 2.105515\n",
      "train epoch: 4 [54384/57363 (95%)]\tloss: 2.390045\n",
      "\n",
      "train epoch: 4\t average loss: 2.050257\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    tr_loss = train(model,train_loader,ix2word,word2ix,DEVICE,optimizer,scheduler,epoch)\n",
    "    train_losses.append(tr_loss)\n",
    "    \n",
    "# 保存模型\n",
    "filename = \"model\" + str(time.time()) + \".pth\"\n",
    "torch.save(model.state_dict(), filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VWUQGjcKGTgM"
   },
   "outputs": [],
   "source": [
    "def generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words=None):\n",
    "    # 读取唐诗的第一句\n",
    "    results = list(start_words)\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 设置第一个词为<START>\n",
    "    input = torch.Tensor([word2ix['<START>']]).view(1, 1).long()\n",
    "    input = input.to(DEVICE)\n",
    "    hidden = None\n",
    "    \n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input, hidden)\n",
    "            input = Variable(input.data.new([word2ix[word]])).view(1, 1)\n",
    "\n",
    "    # 生成唐诗\n",
    "    for i in range(max_gen_len):\n",
    "        output, hidden = model(input, hidden)\n",
    "        # 读取第一句\n",
    "        if i < start_word_len:\n",
    "            w = results[i]\n",
    "            input = input.data.new([word2ix[w]]).view(1, 1)\n",
    "        # 生成后面的句子\n",
    "        else:\n",
    "            top_index = output.data[0].topk(1)[1][0].item()\n",
    "            w = ix2word[top_index]\n",
    "            results.append(w)\n",
    "            input = input.data.new([top_index]).view(1, 1)\n",
    "        # 结束标志\n",
    "        if w == '<EOP>':\n",
    "            del results[-1]\n",
    "            break\n",
    "            \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "UyOSaNZEGV4f",
    "outputId": "b166e65d-d664-4199-d84e-244b7b8df252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春江花月夜，秋風吹不足。\n",
      "一朝不可見，一去無人識。\n",
      "一朝不可見，一日無人識。\n",
      "不知君不見，不知心不得。\n",
      "不知君不見，不知心不得。\n",
      "不知君不見，不知心不得。\n",
      "不知君不見，不知心不得。\n",
      "不知君不見，不知心不得。\n",
      "不知君不見，不知心不得。\n",
      "不知君不見，不知心不得。\n",
      "不知君不得，不知\n"
     ]
    }
   ],
   "source": [
    "start_words = '春江花月夜'  # 唐诗的第一句\n",
    "max_gen_len = 128        # 生成唐诗的最长长度\n",
    "\n",
    "prefix_words = None\n",
    "results = generate(model, start_words, ix2word, word2ix, max_gen_len, prefix_words)\n",
    "poetry = ''\n",
    "for word in results:\n",
    "    poetry += word\n",
    "    if word == '。' or word == '!':\n",
    "        poetry += '\\n'\n",
    "        \n",
    "print(poetry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGPXtqyuGXwj"
   },
   "outputs": [],
   "source": [
    "def gen_acrostic(model, start_words, ix2word, word2ix, prefix_words=None):\n",
    "    # 读取唐诗的“头”\n",
    "    results = []\n",
    "    start_word_len = len(start_words)\n",
    "    \n",
    "    # 设置第一个词为<START>\n",
    "    input = (torch.Tensor([word2ix['<START>']]).view(1, 1).long())\n",
    "    input = input.to(DEVICE)\n",
    "    hidden = None\n",
    "\n",
    "    index = 0            # 指示已生成了多少句\n",
    "    pre_word = '<START>' # 上一个词\n",
    "    \n",
    "    if prefix_words:\n",
    "        for word in prefix_words:\n",
    "            output, hidden = model(input, hidden)\n",
    "            input = Variable(input.data.new([word2ix[word]])).view(1, 1)\n",
    "\n",
    "    # 生成藏头诗\n",
    "    for i in range(max_gen_len_acrostic):\n",
    "        output, hidden = model(input, hidden)\n",
    "        top_index = output.data[0].topk(1)[1][0].item()\n",
    "        w = ix2word[top_index]\n",
    "\n",
    "        # 如果遇到标志一句的结尾，喂入下一个“头”\n",
    "        if (pre_word in {u'。', u'！', '<START>'}):\n",
    "            # 如果生成的诗已经包含全部“头”，则结束\n",
    "            if index == start_word_len:\n",
    "                break\n",
    "            # 把“头”作为输入喂入模型\n",
    "            else:\n",
    "                w = start_words[index]\n",
    "                index += 1\n",
    "                input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "                \n",
    "        # 否则，把上一次预测作为下一个词输入\n",
    "        else:\n",
    "            input = (input.data.new([word2ix[w]])).view(1, 1)\n",
    "        results.append(w)\n",
    "        pre_word = w\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "BTMzGaXLGaxO",
    "outputId": "ba0a9992-73ba-41c1-a44f-0c1cf221b21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "春風吹白雲，萬里無人見。\n",
      "江南水悠悠，江上雲中宿。\n",
      "花落不可見，雲中不可見。\n",
      "月明春草綠，月色寒雲起。\n",
      "夜雨照寒泉，秋風入寒水。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_words_acrostic = '春江花月夜'  # 唐诗的“头”\n",
    "max_gen_len_acrostic = 120               # 生成唐诗的最长长度\n",
    "prefix_words = None\n",
    "results_acrostic = gen_acrostic(model, start_words_acrostic, ix2word, word2ix,prefix_words)\n",
    "\n",
    "poetry = ''\n",
    "for word in results_acrostic:\n",
    "    poetry += word\n",
    "    if word == '。' or word == '!':\n",
    "        poetry += '\\n'\n",
    "        \n",
    "print(poetry)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
